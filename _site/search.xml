<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[记ffmpeg在iOS10下硬编码视频崩溃问题]]></title>
      <url>/ffmpeg/2019/01/10/ffmpeg01/</url>
      <content type="text"><![CDATA[最近使用ffmpeg调用videotoolbox进行硬编码，在iOS10系统下测试时，发现运行一两分钟就会崩溃。同时控制台会输出  [h264_videotoolbox @ 0x12c92d600] Error encoding frame: 0[h264_videotoolbox @ 0x12c92d600]经过查阅ffmpeg源码和videotoolbox相关文档，解决了这个问题，记录下解决过程。ffmpeg调用videotoolbox硬编码简单流程	//找到硬编码codec	AVCodec*codec = avcodec_find_encoder_by_name("h264_videotoolbox");	//创建codecContext	AVCodecContext*ctx = avcodec_alloc_context3(codec);	//设置参数	...	//创建AVFrame	AVFrame *pYuvFrame = av_frame_alloc();	//拷贝原始数据给frame	av_image_fill_arrays(pYuvFrame-&gt;data, pYuvFrame-&gt;linesize, videoBuffer, AV_PIX_FMT_YUV420P, width, height, 1);	//将frame交给context编码	int result = avcodec_send_frame(ctx, pYuvFrame);    if (result &lt; 0) {        //编码错误，释放context        avcodec_free_context(&amp;ctx);        ...        return false;    }    //没有错误，取出编码后数据，做后续处理    AVPacket *pkt = av_packet_alloc();    while (true) {    	//将编码后数据写入AVPacket        result = avcodec_receive_packet(ctx, pkt);        if (result &lt; 0) {            break;        }                ...    }ffmpeg调用videotoolbox硬编码的代码调用和普通的软编码类似，只是avcodec的获取方式通过字符串”h264_videotoolbox”来获取，然后根据codec创建AVCodecContext，在codecContext设置好一系列编码参数后，创建一个AVFrame，调用av_image_fill_arrays将原始视频数据写入AVFrame中，然后调用avcodec_send_frame函数将AVFrame传给codecContext进行编码。定位错误首先根据崩溃线程所在队列名称，可以看出是和videotoolbox硬编码相关的，如果了解过videotoolbox使用流程的话，基本可以确定是在硬编码的回调函数中。打开ffmpeg中videotoolbox硬编码的实现文件videotoolboxenc.c，找到硬编码的回调函数vtenc_output_callback，可以看到崩溃是控制台输出的位置，基本确定崩溃是在vtenc_output_callback中。static void vtenc_output_callback(    void *ctx,    void *sourceFrameCtx,    OSStatus status,    VTEncodeInfoFlags flags,    CMSampleBufferRef sample_buffer){    AVCodecContext *avctx = ctx;    VTEncContext   *vtctx = avctx-&gt;priv_data;    ExtraSEI *sei = sourceFrameCtx;    if (vtctx-&gt;async_error) {        if(sample_buffer) {            CFRelease(sample_buffer);        }        return;    }    if (status || !sample_buffer) {        av_log(avctx, AV_LOG_ERROR, "Error encoding frame: %d\n", (int)status);        set_async_error(vtctx, AVERROR_EXTERNAL);    }   ...    vtenc_q_push(vtctx, sample_buffer, sei);}分析vtenc_output_callback函数这个回调函数首先检查了VTEncContext这个结构体的async_error值是否不为0，这个async_error是用于记录错误值，并最终返回为avcodec_send_frame函数的结果。之后是检查status和sample_buffer,根据控制台输出，可以判断在崩溃前，函数进入到了这个if判断，并且由于status打印的是0，可以知道进入这里是sample_buffer为空。最后将sample_buffer传入一个队列，用于后续的处理。通过增加打印日志找到崩溃前的代码执行顺序回调函数中打印日志static void vtenc_output_callback(    void *ctx,    void *sourceFrameCtx,    OSStatus status,    VTEncodeInfoFlags flags,    CMSampleBufferRef sample_buffer){    printf("vtencoutputcallback %d\n",__LINE__);//522    AVCodecContext *avctx = ctx;    VTEncContext   *vtctx = avctx-&gt;priv_data;    ExtraSEI *sei = sourceFrameCtx;    if (vtctx-&gt;async_error) {        printf("vtencoutputcallback %d\n",__LINE__);//528        if(sample_buffer) {            CFRelease(sample_buffer);        }        printf("vtencoutputcallback %d\n",__LINE__);//534        return;    }    if (status || !sample_buffer) {        av_log(avctx, AV_LOG_ERROR, "Error encoding frame: %d\n", (int)status);        set_async_error(vtctx, AVERROR_EXTERNAL);                printf("vtencoutputcallback %d\n",__LINE__);//542        return;    }    ...    vtenc_q_push(vtctx, sample_buffer, sei);}首先在进入回调后增加打印，然后在出错误的两个判断中也增加，然后重新编译，运行程序。发生崩溃后，打印日志如下：  vtencoutputcallback 522vtencoutputcallback 522Error encoding frame: 0vtencoutputcallback 542[h264_videotoolbox @ 0x14e10be00] vtencoutputcallback 522vtencoutputcallback 528vtencoutputcallback 534            根据日志打印顺序可以分析出，一开始是正常的执行回调函数，直到sample_buffer某次为空后，会先进入(status             !sample_buffer)这个判断并打印Error encoding frame: 0，然后下次进入回调的时候，再进入(vtctx-&gt;async_error)这个判断，然后执行完CFRelease(sample_buffer)后崩溃。      从async_error入手因为崩溃的时候async_error不为0，所以想到看看async_error会影响哪些地方。static int vtenc_q_pop(VTEncContext *vtctx, bool wait, CMSampleBufferRef *buf, ExtraSEI **sei){    BufNode *info;    pthread_mutex_lock(&amp;vtctx-&gt;lock);    if (vtctx-&gt;async_error) {        pthread_mutex_unlock(&amp;vtctx-&gt;lock);        printf("vtencqpop %d",__LINE__);//241        return vtctx-&gt;async_error;    }    ...}static av_cold int vtenc_frame(    AVCodecContext *avctx,    AVPacket       *pkt,    const AVFrame  *frame,    int            *got_packet){    ...    status = vtenc_q_pop(vtctx, !frame, &amp;buf, &amp;sei);    if (status)    {        printf("vtencframe %d status %d\n",__LINE__,status);//2283        goto end_nopkt;    }    if (!buf)   goto end_nopkt;    ...    return 0;end_nopkt:	printf("vtencframe %d end_nopkt\n",__LINE__,status);//2301    av_packet_unref(pkt);    return status;}然后在调用avcodec_send_frame处打印出错时的结果。	int result = avcodec_send_frame(ctx, pYuvFrame);    if (result &lt; 0) {    	printf("result = %d\n",result);        //编码错误，释放context        avcodec_free_context(&amp;ctx);        ...        return false;    }崩溃后，日志输出：  vtencoutputcallback 522vtencoutputcallback 522vtencoutputcallback 522Error encoding frame: 0vtencoutputcallback 542[h264_videotoolbox @ 0x157101e00] vtencqpop 241vtencframe 2283 status -542398533vtencframe 2301 end_nopktresult = -542398533vtencoutputcallback 522vtencoutputcallback 528vtencoutputcallback 534可以看到，在出现空的sample_buffer后，就将错误值返回给了上层调用，然后在上层已经调用 avcodec_free_context(&amp;ctx);释放context的情况下，再次进入了编码回调，并且进入(vtctx-&gt;async_error)这个判断。因此就要看看在释放context的过程中发生了什么。分析AVCodecContext释放过程static av_cold int vtenc_close(AVCodecContext *avctx){    	VTEncContext *vtctx = avctx-&gt;priv_data;    printf("vtencclose %d\n",__LINE__);//2412    if(!vtctx-&gt;session) return 0;    printf("vtencclose %d\n",__LINE__);//2414    VTCompressionSessionCompleteFrames(vtctx-&gt;session,                                       kCMTimeIndefinite);    printf("vtencclose %d\n",__LINE__);//2417    clear_frame_queue(vtctx);	 ...    return 0;}在编码器关闭函数vtenc_close中，每行增加打印,运行程序,崩溃后控制台输出：  vtencoutputcallback 522vtencoutputcallback 522Error encoding frame: 0vtencoutputcallback 542[h264_videotoolbox @ 0x12604aa00] vtencqpop 241vtencframe 2283 status -542398533vtencframe 2301 end_nopktresult = -542398533vtencclose 2412vtencclose 2414vtencoutputcallback 522vtencoutputcallback 528vtencoutputcallback 534根据打印可以看出，执行到  VTCompressionSessionCompleteFrames后就崩溃在CFRelease(sample_buffer);之后。尝试注释CFRelease(sample_buffer);解决问题查找了苹果的官方文档，VTCompressionSessionCompleteFrames的作用是强制让编码器完成编码。  Forces the compression session to complete the encoding of frames.猜测是在回调中sample_buffer已经被释放，而编码器执行了VTCompressionSessionCompleteFrames方法后，可能在回调结束后又对sample_buffer指针发送消息，因此造成野指针崩溃。同时也查阅硬件编码的一些使用示例，确认在编码回调函数中CFRelease(sample_buffer)是不必要的。因此尝试注释这句话，重新运行后不再崩溃。iOS系统差异以及一点优化为什么只在iOS10及以下系统中出现这个问题呢?导致这个问题出现的首要原因是某些情况下sample_buffer会为空，经过测试，在iOS11和12中都不会有这样的情况。而在出现这种情况后，ffmpeg的回调函数是记录一个错误然后返回错误值给调用方，然后调用方销毁context再重新创建，这样对性能是一个损耗。其实在sample_buffer为空的时候不处理，后续不为空的时候继续即可，这样就避免了无谓的销毁和创建context，最终修改的代码如下：static void vtenc_output_callback(    void *ctx,    void *sourceFrameCtx,    OSStatus status,    VTEncodeInfoFlags flags,    CMSampleBufferRef sample_buffer){        if (!CMSampleBufferDataIsReady(sampleBuffer))    {//        NSLog(@"didCompressH264 data is not ready ");        return;    }    AVCodecContext *avctx = ctx;    VTEncContext   *vtctx = avctx-&gt;priv_data;    ExtraSEI *sei = sourceFrameCtx;    if (vtctx-&gt;async_error) {        if(sample_buffer) {//            CFRelease(sample_buffer);        }        return;    }   ...}补充：一个有点弯路的解决方法在找到正确的解决方法之前，还曾经做过一个尝试：既然出错后销毁会有问题，那我不销毁，继续使用context呢？于是经过尝试，发现vtctx-&gt;async_error会一直不为0，也就是说设置过一次后，就无法继续，只有在关闭的时候才会置0，因此尝试在将结果返回给调用方时，也将vtctx-&gt;async_error置为0。代码如下：static int vtenc_q_pop(VTEncContext *vtctx, bool wait, CMSampleBufferRef *buf, ExtraSEI **sei){    BufNode *info;    pthread_mutex_lock(&amp;vtctx-&gt;lock);    if (vtctx-&gt;async_error) {                int err = vtctx-&gt;async_error;        vtctx-&gt;async_error = 0;        pthread_mutex_unlock(&amp;vtctx-&gt;lock);        return err;    }    ...}这样之后，经过尝试，也不会崩溃。但一来这种改动违背了作者的初衷，既然作者没有将vtctx-&gt;async_error置为0，也就是说作者希望出错后就重新创建，二来可能对其他的错误处理造成影响，因此不考虑使用这种方式解决。]]></content>
      <categories>
        
          <category> ffmpeg </category>
        
      </categories>
      <tags>
        
          <tag> ffmpeg </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[GPUImage3源码研究 研究视频滤镜过程]]></title>
      <url>/gpuimage/2018/11/07/GPUImage3_01/</url>
      <content type="text"><![CDATA[GPUImage是使用OpenGL进行滤镜处理的框架，作者后续开发了使用swift+OpenGL的GPUImage2，后续又有基于Metal的GPUImage3，之前学习了下Metal的一些基本使用，这次来学习下GPUImage3的源码。采集视频经过滤镜渲染至界面import UIKitimport GPUImageimport AVFoundationclass ViewController: UIViewController {    @IBOutlet weak var renderView: RenderView!        var camera:Camera!    override func viewDidLoad() {        super.viewDidLoad()        // Do any additional setup after loading the view.                do {            camera = try Camera(sessionPreset:.hd1280x720, cameraDevice: nil, location: .frontFacing, captureAsYUV: true)            let filter = SketchFilter()            camera --&gt; filter --&gt; renderView            camera.startCapture()        } catch {            fatalError("Could not initialize rendering pipeline: \(error)")        }    }}这段代码创建了一个Camera对象，一个滤镜对象，以及在Storyboard上添加的一个RenderView，然后使用–&gt;这个运算符将三者作为一个响应链。infix operator --&gt; : AdditionPrecedence@discardableResult public func --&gt;&lt;T:ImageConsumer&gt;(source:ImageSource, destination:T) -&gt; T {    source.addTarget(destination)    return destination}–&gt;实际上是一个自定义运算符，它将右边的对象添加到左边对象的targets中，然后返回右边的对象，这样可以连续使用。swift自定义运算符分析代码Camera对象public init(sessionPreset:AVCaptureSession.Preset, cameraDevice:AVCaptureDevice? = nil, location:PhysicalCameraLocation = .backFacing, captureAsYUV:Bool = true) throws {        self.location = location                self.captureSession = AVCaptureSession()        self.captureSession.beginConfiguration()                self.captureAsYUV = captureAsYUV                if let cameraDevice = cameraDevice {            self.inputCamera = cameraDevice        } else {            if let device = location.device() {                self.inputCamera = device            } else {                self.videoInput = nil                self.videoOutput = nil                self.inputCamera = nil                self.yuvConversionRenderPipelineState = nil                super.init()                throw CameraError()            }        }                do {            self.videoInput = try AVCaptureDeviceInput(device:inputCamera)        } catch {            self.videoInput = nil            self.videoOutput = nil            self.yuvConversionRenderPipelineState = nil            super.init()            throw error        }                if (captureSession.canAddInput(videoInput)) {            captureSession.addInput(videoInput)        }                // Add the video frame output        videoOutput = AVCaptureVideoDataOutput()        videoOutput.alwaysDiscardsLateVideoFrames = false				...}这里首先创建了一些视频采集所需要的对象，如AVCaptureSession等。public init(sessionPreset:AVCaptureSession.Preset, cameraDevice:AVCaptureDevice? = nil, location:PhysicalCameraLocation = .backFacing, captureAsYUV:Bool = true) throws {		...		if captureAsYUV {            supportsFullYUVRange = false            let supportedPixelFormats = videoOutput.availableVideoPixelFormatTypes            for currentPixelFormat in supportedPixelFormats {                if ((currentPixelFormat as NSNumber).int32Value == Int32(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange)) {                    supportsFullYUVRange = true                }            }            if (supportsFullYUVRange) {                let (pipelineState, lookupTable) = generateRenderPipelineState(device:sharedMetalRenderingDevice, vertexFunctionName:"twoInputVertex", fragmentFunctionName:"yuvConversionFullRangeFragment", operationName:"YUVToRGB")                self.yuvConversionRenderPipelineState = pipelineState                self.yuvLookupTable = lookupTable                videoOutput.videoSettings = [kCVPixelBufferMetalCompatibilityKey as String: true,                                             kCVPixelBufferPixelFormatTypeKey as String:NSNumber(value:Int32(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange))]            } else {                let (pipelineState, lookupTable) = generateRenderPipelineState(device:sharedMetalRenderingDevice, vertexFunctionName:"twoInputVertex", fragmentFunctionName:"yuvConversionVideoRangeFragment", operationName:"YUVToRGB")                self.yuvConversionRenderPipelineState = pipelineState                self.yuvLookupTable = lookupTable                videoOutput.videoSettings = [kCVPixelBufferMetalCompatibilityKey as String: true,                                             kCVPixelBufferPixelFormatTypeKey as String:NSNumber(value:Int32(kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange))]            }        } else {            self.yuvConversionRenderPipelineState = nil            videoOutput.videoSettings = [kCVPixelBufferMetalCompatibilityKey as String: true,                                         kCVPixelBufferPixelFormatTypeKey as String:NSNumber(value:Int32(kCVPixelFormatType_32BGRA))]        }				...}这里根据设置的是否采集yuv标记，来设置采集的视频数据格式，如果是采集yuv，还需要判断是否是全范围的yuv（fullRange）,设置完毕后，如果是yuv格式，还会创建一个用于转换yuv到rgb的MTLRenderPipelineState,sharedMetalRenderingDevice是一个全局的设备，是对MTLDevice的一个封装，里面包含了MTLCommandQueue、MTLLibrary。如果是采集rgba，则直接设置视频格式。public init(sessionPreset:AVCaptureSession.Preset, cameraDevice:AVCaptureDevice? = nil, location:PhysicalCameraLocation = .backFacing, captureAsYUV:Bool = true) throws {		...		if (captureSession.canAddOutput(videoOutput)) {            captureSession.addOutput(videoOutput)        }                captureSession.sessionPreset = sessionPreset        captureSession.commitConfiguration()                super.init()                let _ = CVMetalTextureCacheCreate(kCFAllocatorDefault, nil, sharedMetalRenderingDevice.device, nil, &amp;videoTextureCache)        videoOutput.setSampleBufferDelegate(self, queue:cameraProcessingQueue)}最后创建了一个Metal的纹理缓存。滤镜对象这里使用的SketchFilter滤镜，它是继承自TextureSamplingOperation的子类，TextureSamplingOperation继承自BasicOperation，BasicOperation遵循了ImageProcessingOperation协议。ImageProcessingOperation协议又遵循了ImageConsumer，ImageSource，表明BasicOperation既可以是图像数据的来源，也可以是消费者，这样就可以构成响应链的一环。public init(vertexFunctionName: String? = nil, fragmentFunctionName: String, numberOfInputs: UInt = 1, operationName: String = #file) {        self.maximumInputs = numberOfInputs        self.operationName = operationName                let concreteVertexFunctionName = vertexFunctionName ?? defaultVertexFunctionNameForInputs(numberOfInputs)        let (pipelineState, lookupTable) = generateRenderPipelineState(device:sharedMetalRenderingDevice, vertexFunctionName:concreteVertexFunctionName, fragmentFunctionName:fragmentFunctionName, operationName:operationName)        self.renderPipelineState = pipelineState        self.uniformSettings = ShaderUniformSettings(uniformLookupTable:lookupTable)    }这里是BasicOperation的初始化函数，首先根据外部传入一个输入（纹理）数量，默认是1，使用operationName保存执行对象的文件路径，用于记录错误信息。然后判断是否传入顶点着色器函数名，没有的话根据输入来确定默认的顶点着色器函数名。之后根据传入的片段着色器函数名和顶点着色器函数名创建一个用于滤镜处理的MTLRenderPipelineState。RenderViewRenderView是继承自MTKView，并且遵守了了ImageConsumer协议，使它可以作为数据的消费者，响应链的终点。public override init(frame frameRect: CGRect, device: MTLDevice?) {        super.init(frame: frameRect, device: sharedMetalRenderingDevice.device)                commonInit()    }        public required init(coder: NSCoder) {        super.init(coder: coder)                commonInit()    }        private func commonInit() {        framebufferOnly = false        autoResizeDrawable = true                self.device = sharedMetalRenderingDevice.device                let (pipelineState, _) = generateRenderPipelineState(device:sharedMetalRenderingDevice, vertexFunctionName:"oneInputVertex", fragmentFunctionName:"passthroughFragment", operationName:"RenderView")        self.renderPipelineState = pipelineState                enableSetNeedsDisplay = false        isPaused = true    }RenderView初始化时也创建了一个用于渲染数据的MTLRenderPipelineState。创建MTLRenderPipelineStatefunc generateRenderPipelineState(device:MetalRenderingDevice, vertexFunctionName:String, fragmentFunctionName:String, operationName:String) -&gt; (MTLRenderPipelineState, [String:(Int, MTLDataType)]) {    guard let vertexFunction = device.shaderLibrary.makeFunction(name: vertexFunctionName) else {        fatalError("\(operationName): could not compile vertex function \(vertexFunctionName)")    }        guard let fragmentFunction = device.shaderLibrary.makeFunction(name: fragmentFunctionName) else {        fatalError("\(operationName): could not compile fragment function \(fragmentFunctionName)")    }        let descriptor = MTLRenderPipelineDescriptor()    descriptor.colorAttachments[0].pixelFormat = MTLPixelFormat.bgra8Unorm    descriptor.rasterSampleCount = 1    descriptor.vertexFunction = vertexFunction    descriptor.fragmentFunction = fragmentFunction        do {        var reflection:MTLAutoreleasedRenderPipelineReflection?        let pipelineState = try device.device.makeRenderPipelineState(descriptor: descriptor, options: [.bufferTypeInfo, .argumentInfo], reflection: &amp;reflection)        var uniformLookupTable:[String:(Int, MTLDataType)] = [:]        if let fragmentArguments = reflection?.fragmentArguments {            for fragmentArgument in fragmentArguments where fragmentArgument.type == .buffer {                if (fragmentArgument.bufferDataType == .struct) {                    for (index, uniform) in fragmentArgument.bufferStructType.members.enumerated() {                        uniformLookupTable[uniform.name] = (index, uniform.dataType)                    }                }            }        }                return (pipelineState, uniformLookupTable)    } catch {        fatalError("Could not create render pipeline state for vertex:\(vertexFunctionName), fragment:\(fragmentFunctionName), error:\(error)")    }}在创建MTLRenderPipelineState的函数中，首先创建顶点着色器和片段着色器两个MTLFunction，然后创建一个管道状态描述器（MTLRenderPipelineDescriptor），给这个描述器设置函数。然后使用全局的MTLDevice，创建管道状态（MTLRenderPipelineState），最后使用MTLRenderPipelineReflection获取片段着色器参数，将类型为buffer的参数存到一个字典中，再将管道状态和参数字典返回。Camera采集到视频数据后的处理public func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {		...		let cameraFrame = CMSampleBufferGetImageBuffer(sampleBuffer)!        let bufferWidth = CVPixelBufferGetWidth(cameraFrame)        let bufferHeight = CVPixelBufferGetHeight(cameraFrame)        let _ = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)                CVPixelBufferLockBaseAddress(cameraFrame, CVPixelBufferLockFlags(rawValue:CVOptionFlags(0)))				cameraFrameProcessingQueue.async {			...		}}在视频数据的回调中，首先取得CMSampleBuffer的pixelBuffer，然后取得宽高。public func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {               ...        cameraFrameProcessingQueue.async {            self.delegate?.didCaptureBuffer(sampleBuffer)            CVPixelBufferUnlockBaseAddress(cameraFrame, CVPixelBufferLockFlags(rawValue:CVOptionFlags(0)))                        let texture:Texture?            if self.captureAsYUV {                var luminanceTextureRef:CVMetalTexture? = nil                var chrominanceTextureRef:CVMetalTexture? = nil                // Luminance plane                let _ = CVMetalTextureCacheCreateTextureFromImage(kCFAllocatorDefault, self.videoTextureCache!, cameraFrame, nil, .r8Unorm, bufferWidth, bufferHeight, 0, &amp;luminanceTextureRef)                // Chrominance plane                let _ = CVMetalTextureCacheCreateTextureFromImage(kCFAllocatorDefault, self.videoTextureCache!, cameraFrame, nil, .rg8Unorm, bufferWidth / 2, bufferHeight / 2, 1, &amp;chrominanceTextureRef)                                if let concreteLuminanceTextureRef = luminanceTextureRef, let concreteChrominanceTextureRef = chrominanceTextureRef,                    let luminanceTexture = CVMetalTextureGetTexture(concreteLuminanceTextureRef), let chrominanceTexture = CVMetalTextureGetTexture(concreteChrominanceTextureRef) {                                        let conversionMatrix:Matrix3x3                    if (self.supportsFullYUVRange) {                        conversionMatrix = colorConversionMatrix601FullRangeDefault                    } else {                        conversionMatrix = colorConversionMatrix601Default                    }                                        let outputWidth:Int                    let outputHeight:Int                    if self.location.imageOrientation().rotationNeeded(for:.portrait).flipsDimensions() {                        outputWidth = bufferHeight                        outputHeight = bufferWidth                    } else {                        outputWidth = bufferWidth                        outputHeight = bufferHeight                    }                    let outputTexture = Texture(device:sharedMetalRenderingDevice.device, orientation:.portrait, width:outputWidth, height:outputHeight)                                        convertYUVToRGB(pipelineState:self.yuvConversionRenderPipelineState!, lookupTable:self.yuvLookupTable,                                    luminanceTexture:Texture(orientation: self.location.imageOrientation(), texture:luminanceTexture),                                    chrominanceTexture:Texture(orientation: self.location.imageOrientation(), texture:chrominanceTexture),                                    resultTexture:outputTexture, colorConversionMatrix:conversionMatrix)                    texture = outputTexture                } else {                    texture = nil                }            } else {                var textureRef:CVMetalTexture? = nil                let _ = CVMetalTextureCacheCreateTextureFromImage(kCFAllocatorDefault, self.videoTextureCache!, cameraFrame, nil, .bgra8Unorm, bufferWidth, bufferHeight, 0, &amp;textureRef)                if let concreteTexture = textureRef, let cameraTexture = CVMetalTextureGetTexture(concreteTexture) {                    texture = Texture(orientation: self.location.imageOrientation(), texture: cameraTexture)                } else {                    texture = nil                }            }                        if texture != nil {                self.updateTargetsWithTexture(texture!)            }            ...        }    }接下来判断是否是yuv数据，如果是，则创建两个纹理，存放y通道和uv通道，并且确定一个转换矩阵，之后调用convertYUVToRGB方法，将数据转换为RGB然后写入outputTexture。如果是RGBA数据，则直接写入纹理。之后将纹理传给响应链的下一环进行处理。public func convertYUVToRGB(pipelineState:MTLRenderPipelineState, lookupTable:[String:(Int, MTLDataType)], luminanceTexture:Texture, chrominanceTexture:Texture, secondChrominanceTexture:Texture? = nil, resultTexture:Texture, colorConversionMatrix:Matrix3x3) {    let uniformSettings = ShaderUniformSettings(uniformLookupTable:lookupTable)    uniformSettings["colorConversionMatrix"] = colorConversionMatrix        guard let commandBuffer = sharedMetalRenderingDevice.commandQueue.makeCommandBuffer() else {return}        let inputTextures:[UInt:Texture]    if let secondChrominanceTexture = secondChrominanceTexture {        inputTextures = [0:luminanceTexture, 1:chrominanceTexture, 2:secondChrominanceTexture]    } else {        inputTextures = [0:luminanceTexture, 1:chrominanceTexture]    }        commandBuffer.renderQuad(pipelineState:pipelineState, uniformSettings:uniformSettings, inputTextures:inputTextures, useNormalizedTextureCoordinates:true, outputTexture:resultTexture)    commandBuffer.commit()}func renderQuad(pipelineState:MTLRenderPipelineState, uniformSettings:ShaderUniformSettings? = nil, inputTextures:[UInt:Texture], useNormalizedTextureCoordinates:Bool = true, imageVertices:[Float] = standardImageVertices, outputTexture:Texture, outputOrientation:ImageOrientation = .portrait) {        let vertexBuffer = sharedMetalRenderingDevice.device.makeBuffer(bytes: imageVertices,                                                                        length: imageVertices.count * MemoryLayout&lt;Float&gt;.size,                                                                        options: [])!        vertexBuffer.label = "Vertices"                        let renderPass = MTLRenderPassDescriptor()        renderPass.colorAttachments[0].texture = outputTexture.texture        renderPass.colorAttachments[0].clearColor = MTLClearColorMake(1, 0, 0, 1)        renderPass.colorAttachments[0].storeAction = .store        renderPass.colorAttachments[0].loadAction = .clear                guard let renderEncoder = self.makeRenderCommandEncoder(descriptor: renderPass) else {            fatalError("Could not create render encoder")        }        renderEncoder.setFrontFacing(.counterClockwise)        renderEncoder.setRenderPipelineState(pipelineState)        renderEncoder.setVertexBuffer(vertexBuffer, offset: 0, index: 0)                for textureIndex in 0..&lt;inputTextures.count {            let currentTexture = inputTextures[UInt(textureIndex)]!                        let inputTextureCoordinates = currentTexture.textureCoordinates(for:outputOrientation, normalized:useNormalizedTextureCoordinates)            let textureBuffer = sharedMetalRenderingDevice.device.makeBuffer(bytes: inputTextureCoordinates,                                                                             length: inputTextureCoordinates.count * MemoryLayout&lt;Float&gt;.size,                                                                             options: [])!            textureBuffer.label = "Texture Coordinates"            renderEncoder.setVertexBuffer(textureBuffer, offset: 0, index: 1 + textureIndex)            renderEncoder.setFragmentTexture(currentTexture.texture, index: textureIndex)        }        uniformSettings?.restoreShaderSettings(renderEncoder: renderEncoder)        renderEncoder.drawPrimitives(type: .triangleStrip, vertexStart: 0, vertexCount: 4)        renderEncoder.endEncoding()    }}在convertYUVToRGB中，首先创建了一个commandBuffer，然后将纹理存入一个字典中，再调用commandBuffer的一个扩展方法，renderQuad，最后提交commandBuffer，开始真正的渲染过程。在renderQuad方法中，首先根据顶点坐标创建了一个MTLBuffer ，然后创建一个MTLRenderPassDescriptor，将输出的纹理作为colorAttachments[0]的纹理，表示将渲染的结果写入这个纹理。之后创建一个renderEncoder，设置顶点buffer。然后遍历输入纹理字典，将y通道和uv通道的纹理按索引传入renderEncoder。之后再将uniform参数字典遍历，将相应的参数写入renderEncoder。最后调用绘制函数。public func updateTargetsWithTexture(_ texture:Texture) {        for (target, index) in targets {            target.newTextureAvailable(texture, fromSourceIndex:index)        }    }在采集的纹理转换好之后，Camera会调用updateTargetsWithTexture方法，遍历所有的targets，将纹理传给target。target会调用newTextureAvailable方法。filter的newTextureAvailablepublic func newTextureAvailable(_ texture: Texture, fromSourceIndex: UInt) {        ...                inputTextures[fromSourceIndex] = texture        ...                if (UInt(inputTextures.count) &gt;= maximumInputs) {            let outputWidth:Int            let outputHeight:Int                        let firstInputTexture = inputTextures[0]!            if firstInputTexture.orientation.rotationNeeded(for:.portrait).flipsDimensions() {                outputWidth = firstInputTexture.texture.height                outputHeight = firstInputTexture.texture.width            } else {                outputWidth = firstInputTexture.texture.width                outputHeight = firstInputTexture.texture.height            }            if uniformSettings.usesAspectRatio {                let outputRotation = firstInputTexture.orientation.rotationNeeded(for:.portrait)                uniformSettings["aspectRatio"] = firstInputTexture.aspectRatio(for: outputRotation)            }                        ...        }    }这里收先将传入的texture写入输入纹理数组。然后判断输入纹理数量是否超出最大限制，超出就什么都不做。然后根据纹理的宽高、缩放方式等，算出一个输出的宽高。public func newTextureAvailable(_ texture: Texture, fromSourceIndex: UInt) {        ...        	guard let commandBuffer = sharedMetalRenderingDevice.commandQueue.makeCommandBuffer() else {return}            let outputTexture = Texture(device:sharedMetalRenderingDevice.device, orientation: .portrait, width: outputWidth, height: outputHeight)                        if let alternateRenderingFunction = metalPerformanceShaderPathway, useMetalPerformanceShaders {                var rotatedInputTextures: [UInt:Texture]                if (firstInputTexture.orientation.rotationNeeded(for:.portrait) != .noRotation) {                    let rotationOutputTexture = Texture(device:sharedMetalRenderingDevice.device, orientation: .portrait, width: outputWidth, height: outputHeight)                    guard let rotationCommandBuffer = sharedMetalRenderingDevice.commandQueue.makeCommandBuffer() else {return}                    rotationCommandBuffer.renderQuad(pipelineState: sharedMetalRenderingDevice.passthroughRenderState, uniformSettings: uniformSettings, inputTextures: inputTextures, useNormalizedTextureCoordinates: useNormalizedTextureCoordinates, outputTexture: rotationOutputTexture)                    rotationCommandBuffer.commit()                    rotatedInputTextures = inputTextures                    rotatedInputTextures[0] = rotationOutputTexture                } else {                    rotatedInputTextures = inputTextures                }                alternateRenderingFunction(commandBuffer, rotatedInputTextures, outputTexture)            } else {                internalRenderFunction(commandBuffer: commandBuffer, outputTexture: outputTexture)            }            commandBuffer.commit()                        updateTargetsWithTexture(outputTexture)}	func internalRenderFunction(commandBuffer: MTLCommandBuffer, outputTexture: Texture) {        commandBuffer.renderQuad(pipelineState: renderPipelineState, uniformSettings: uniformSettings, inputTextures: inputTextures, useNormalizedTextureCoordinates: useNormalizedTextureCoordinates, outputTexture: outputTexture)    }然后创建一个commandBuffer和一个用于接受输出的纹理对象。之后判断是否有一个替代的渲染函数，默认没有，然后执行默认的渲染函数，实际就是执行commandBuffer的renderQuad函数。然后提交commandBuffer，执行渲染。最后再寻找响应链的下一个target，将输出的纹理传给它。RenderView的newTextureAvailablepublic func newTextureAvailable(_ texture:Texture, fromSourceIndex:UInt) {        self.drawableSize = CGSize(width: texture.texture.width, height: texture.texture.height)        currentTexture = texture        self.draw()    }        public override func draw(_ rect:CGRect) {        if let currentDrawable = self.currentDrawable, let imageTexture = currentTexture {            let commandBuffer = sharedMetalRenderingDevice.commandQueue.makeCommandBuffer()                        let outputTexture = Texture(orientation: .portrait, texture: currentDrawable.texture)            commandBuffer?.renderQuad(pipelineState: renderPipelineState, inputTextures: [0:imageTexture], outputTexture: outputTexture)                        commandBuffer?.present(currentDrawable)            commandBuffer?.commit()        }    }这里调用draw方法，然后在draw方法中也是创建commandBuffer，然后调用commandBuffer的renderQuad方法，不过输出的纹理使用的是currentDrawable的纹理，它是MTKView的属性用于渲染显示。总结GPUImage3也是和GPUImage类似的滤镜链处理，每个滤镜链上的对象都有一个MTLRenderPipelineState，用于做各自的滤镜处理。输入和输出被抽象成两个协议ImageSource和ImageConsumer，每次渲染时会生成一个commandBuffer然后提交渲染命令。感觉这种响应链的模式很值得学习。同时也觉得Metal的代码比OpenGL要简练一些，更符合iOS开发者的习惯。]]></content>
      <categories>
        
          <category> GPUImage </category>
        
      </categories>
      <tags>
        
          <tag> OpenGL </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
          <tag> GPUImage </tag>
        
          <tag> Metal </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Metal学习笔记03 渲染视频原始数据]]></title>
      <url>/metal/2018/10/30/Metal03/</url>
      <content type="text"><![CDATA[上篇笔记这次来使用Metal渲染视频数据，渲染视频实际上就是不断地把视频数据变成MTLTexture，再使用renderEncoder去渲染显示，本文会利用iOS设备采集RGBA、NV12数据，然后进行渲染显示。渲染BGRA数据创建一个layer类型为CAMetalLayer的view+(Class)layerClass{    return [CAMetalLayer class];}设置layer的属性@property(nonatomic,strong)id&lt;MTLDevice&gt;device;@property(nonatomic,strong)CAMetalLayer * metalLayer;...-(void)prepareLayer{    self.device = MTLCreateSystemDefaultDevice();            self.metalLayer = (CAMetalLayer*)self.layer;        self.metalLayer.pixelFormat = MTLPixelFormatBGRA8Unorm;    self.metalLayer.framebufferOnly = YES;    self.metalLayer.drawableSize = self.bounds.size;    self.metalLayer.device = self.device;    }这里主要是获取了默认的设备（GPU）然后赋值给layer的device属性，然后设置一些渲染格式和尺寸的属性。准备管道状态@property(nonatomic,strong)id&lt;MTLRenderPipelineState&gt; pipelineState;...-(void)preparePipelineState{    id&lt;MTLLibrary&gt;library = [self.device newDefaultLibrary];    id&lt;MTLFunction&gt;vertexFunc = [library newFunctionWithName:@"texture_vertex"];    id&lt;MTLFunction&gt;fragmentFunc = [library newFunctionWithName:@"texture_fragment"];        MTLRenderPipelineDescriptor*descriptor = [[MTLRenderPipelineDescriptor alloc]init];    descriptor.vertexFunction = vertexFunc;    descriptor.fragmentFunction = fragmentFunc;    descriptor.colorAttachments[0].pixelFormat = MTLPixelFormatBGRA8Unorm;        id&lt;MTLRenderPipelineState&gt; pipelineState = [self.device newRenderPipelineStateWithDescriptor:descriptor error:nil];        self.pipelineState = pipelineState;    }#include &lt;metal_stdlib&gt;using namespace metal;struct VertexOut{    float4 position [[position]];    float2 textureCoordinate;};vertex VertexOut texture_vertex (    constant float4*vertex_array[[buffer(0)]],    constant float2*textureCoord_array[[buffer(1)]],    unsigned int vid[[vertex_id]]){    VertexOut outputVertices;    outputVertices.position = vertex_array[vid];    outputVertices.textureCoordinate = textureCoord_array[vid];    return outputVertices;}fragment float4 texture_fragment(VertexOut fragmentInput [[stage_in]],                                 texture2d&lt;float&gt; inputTexture [[texture(0)]]) {    constexpr sampler quadSampler;    float4 color = inputTexture.sample(quadSampler, fragmentInput.textureCoordinate);    return color;}这里创建了管道状态，设置了两个着色器函数，和上一篇笔记渲染图片纹理的函数一致。创建命令队列@property(nonatomic,strong)id&lt;MTLCommandQueue&gt;commandQueue;...-(void)prepareCommandQueue{    id&lt;MTLCommandQueue&gt;commandQueue = [self.device newCommandQueue];    self.commandQueue = commandQueue;}这里创建一个命令队列，用于渲染时取得可用的command给GPU渲染命令。渲染方法@property(nonatomic,strong)id&lt;MTLTexture&gt;texture;@property(nonatomic,strong)MTLTextureDescriptor*textureDes;@property(nonatomic,assign)int  textureHeight;@property(nonatomic,assign)int  textureWidth;...-(void)renderRGBAWith:(uint8_t*)RGBBuffer width:(int)width height:(int)height{    if (!self.textureDes || self.textureWidth != width ||self.textureHeight != height) {        self.textureDes = [MTLTextureDescriptor texture2DDescriptorWithPixelFormat:MTLPixelFormatBGRA8Unorm width:width height:height mipmapped:NO];    }	//创建纹理    self.texture = [self.device newTextureWithDescriptor:self.textureDes];    MTLRegion region = MTLRegionMake2D(0, 0, width, height);	//将数据写入纹理    [self.texture replaceRegion:region mipmapLevel:0 withBytes:RGBBuffer bytesPerRow:width*4];                id&lt;MTLCommandBuffer&gt;commandBuffer = [self.commandQueue commandBuffer];        id&lt;CAMetalDrawable&gt;drawable = [self.metalLayer nextDrawable];        MTLRenderPassDescriptor*renderPassDes = [[MTLRenderPassDescriptor alloc]init];        renderPassDes.colorAttachments[0].texture = [drawable texture];    renderPassDes.colorAttachments[0].loadAction = MTLLoadActionClear;    renderPassDes.colorAttachments[0].clearColor = MTLClearColorMake(1.0, 1.0, 1.0, 1.0);    renderPassDes.colorAttachments[0].storeAction = MTLStoreActionStore;        id&lt;MTLRenderCommandEncoder&gt;renderEncoder = [commandBuffer renderCommandEncoderWithDescriptor:renderPassDes];    [renderEncoder setRenderPipelineState:self.pipelineState];            float vertexArray[] = {        -1.0, -1.0,0, 1.0,        1.0, -1.0, 0, 1.0,        -1.0,  1.0, 0, 1.0,        1.0,  1.0, 0, 1.0,    };            //顶点坐标buffer    id&lt;MTLBuffer&gt;vertexBuffer = [self.device newBufferWithBytes:vertexArray length:sizeof(vertexArray) options:MTLResourceCPUCacheModeDefaultCache];        [renderEncoder setVertexBuffer:vertexBuffer offset:0 atIndex:0];            float textureCoord[] = {        0,1,        1,1,        0,0,        1,0    };	//纹理坐标buffer    id&lt;MTLBuffer&gt;textureCoordBuffer = [self.device newBufferWithBytes:textureCoord length:sizeof(textureCoord) options:MTLResourceCPUCacheModeDefaultCache];        [renderEncoder setVertexBuffer:textureCoordBuffer offset:0 atIndex:1];    [renderEncoder setFragmentTexture:self.texture atIndex:0];        [renderEncoder drawPrimitives:MTLPrimitiveTypeTriangleStrip vertexStart:0 vertexCount:4];            [commandBuffer presentDrawable:drawable];        [renderEncoder endEncoding];    [commandBuffer commit];            }这里根据传入的宽高判断是否需要新创建纹理描述，然后根据纹理描述创建纹理，将数据写入。然后创建MTLRenderPassDescriptor，MTLRenderPassDescriptor描述一系列attachments的值，类似GL的FrameBuffer，同时也用来创建MTLRenderCommandEncoder。然后创建顶点和纹理坐标buffer，然后设置给renderEncoder，然后渲染显示。将数据传入renderView-(void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);        if(CVPixelBufferLockBaseAddress(imageBuffer, 0) == kCVReturnSuccess)    {                UInt8 *rgbBuffer = (UInt8 *)CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 0);                size_t width = CVPixelBufferGetWidth(imageBuffer);                size_t height = CVPixelBufferGetHeight(imageBuffer);                [self.renderView renderRGBAWith:rgbBuffer width:width height:height];                    }    CVPixelBufferUnlockBaseAddress(imageBuffer, 0);}运行后即可看到渲染的视频。遇到的问题一开始我在采集之后才将renderView添加到view上，导致崩溃在id&lt;CAMetalDrawable&gt;drawable = [self.metalLayer nextDrawable];原因是drawableSize为0，无法取出drawable。渲染NV12使用两个纹理@property(nonatomic,strong)MTLTextureDescriptor*textureYDes;@property(nonatomic,strong)MTLTextureDescriptor*textureUVDes;@property(nonatomic,strong)id&lt;MTLTexture&gt;textureY;@property(nonatomic,strong)id&lt;MTLTexture&gt;textureUV;...-(void)renderNV12With:(uint8_t*)yBuffer uvBuffer:(uint8_t*)uvBuffer width:(int)width height:(int)height	if (!self.textureYDes || self.textureWidth != width ||self.textureHeight != height) {			self.textureYDes = [MTLTextureDescriptor texture2DDescriptorWithPixelFormat:MTLPixelFormatR8Unorm width:width height:height mipmapped:NO];		}		self.textureY = [self.device newTextureWithDescriptor:self.textureYDes];		MTLRegion region = MTLRegionMake2D(0, 0, width, height);		[self.textureY replaceRegion:region mipmapLevel:0 withBytes:yBuffer bytesPerRow:width];		if (!self.textureUVDes || self.textureWidth != width ||self.textureHeight != height) {			self.textureUVDes = [MTLTextureDescriptor texture2DDescriptorWithPixelFormat:MTLPixelFormatRG8Unorm width:width/2 height:height/2 mipmapped:NO];		}		region = MTLRegionMake2D(0, 0, width/2, height/2);		self.textureUV = [self.device newTextureWithDescriptor:self.textureUVDes];		[self.textureUV replaceRegion:region mipmapLevel:0 withBytes:uvBuffer bytesPerRow:width];		...}由于NV12有两个通道，因此需要传入两个纹理。这里使用两个MTLTextureDescriptor创建，y分量的纹理因为只有y值，因此使用MTLPixelFormatR8Unorm格式，表示一个八位的通道。uv分量使用MTLPixelFormatRG8Unorm，有两个八位通道。修改片段着色器fragment float4 nv12_fragment(VertexOut fragmentInput [[stage_in]],                              texture2d&lt;float&gt; textureY [[texture(0)]]，                               texture2d&lt;float&gt; textureUV [[texture(1)]]) {    constexpr sampler quadSampler;        float y = textureY.sample(quadSampler,fragmentInput.textureCoordinate).r;    float u = textureUV.sample(quadSampler, fragmentInput.textureCoordinate).r - 0.5;        float v = textureUV.sample(quadSampler, fragmentInput.textureCoordinate).g - 0.5;        float r = y +             1.402 * v;    float g = y - 0.344 * u - 0.714 * v;    float b = y + 1.772 * u;        float4 color = float4(r,g,b,1.0);        return color;}这里增加一个纹理参数，从textureY中取出y，textureUV分别取出uv，利用公式转换成rgb值。修改着色器程序//    id&lt;MTLFunction&gt;fragmentFunc = [library newFunctionWithName:@"texture_fragment"];    id&lt;MTLFunction&gt;fragmentFunc = [library newFunctionWithName:@"nv12_fragment"];给renderEncoder传入两个纹理 	[renderEncoder setFragmentTexture:self.textureY atIndex:0];    [renderEncoder setFragmentTexture:self.textureUV atIndex:1];调用渲染方法-(void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);        if(CVPixelBufferLockBaseAddress(imageBuffer, 0) == kCVReturnSuccess)    {                UInt8 *yBuffer = (UInt8 *)CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 0);            UInt8 *uvBuffer = (UInt8 *)CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 1);                size_t width = CVPixelBufferGetWidth(imageBuffer);                size_t height = CVPixelBufferGetHeight(imageBuffer);                        size_t linesize_yu = CVPixelBufferGetBytesPerRowOfPlane(imageBuffer, 1);//        [self.renderView renderRGBAWith:rgbBuffer width:width height:height];        [self.renderView renderNV12With:yBuffer uvBuffer:uvBuffer width:width height:height];                    }    CVPixelBufferUnlockBaseAddress(imageBuffer, 0);}运行程序，即可看到采集的画面。使用CoreVideo相关函数创建纹理使用CVMetalTextureCacheRef纹理缓存，CVMetalTextureCacheCreateTextureFromImage函数可以直接从CVPixelBufferRef中获取CVMetalTextureRef，然后在从CVMetalTextureRef中获得MTLTexture。@property (nonatomic, assign) CVMetalTextureCacheRef textureCache;...//创建CVMetalTextureCacheRefCVMetalTextureCacheCreate(NULL, NULL, self.device, NULL, &amp;_textureCache);...		id&lt;MTLTexture&gt; textureY = nil;        size_t width = CVPixelBufferGetWidthOfPlane(pixelBuffer, 0);        size_t height = CVPixelBufferGetHeightOfPlane(pixelBuffer, 0);        MTLPixelFormat pixelFormat = MTLPixelFormatR8Unorm;	        CVMetalTextureRef texture = NULL; 		//将pixelBuffer的0通道写入CVMetalTextureRef        CVReturn status = CVMetalTextureCacheCreateTextureFromImage(NULL, self.textureCache, pixelBuffer, NULL, pixelFormat, width, height, 0, &amp;texture);        if(status == kCVReturnSuccess)        {            textureY = CVMetalTextureGetTexture(texture); // 转成Metal用的纹理            CFRelease(texture);        }    之后就和上面的纹理处理一致了。总结感觉使用Metal渲染的视频的代码比OpenGL的实现要容易些，基本就是渲染图片纹理稍作修改即可实现，而且MetalKit还提供了一些直接将AVFoundation数据转换为Metal可以处理的类型的功能，使得开发更加简单。demo地址]]></content>
      <categories>
        
          <category> Metal </category>
        
      </categories>
      <tags>
        
          <tag> OpenGL </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
          <tag> Metal </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Metal学习笔记02 渲染图片纹理]]></title>
      <url>/metal/2018/10/23/Metal02/</url>
      <content type="text"><![CDATA[上篇笔记上篇笔记实现了用Metal绘制三角形和矩形，这次来绘制一张图片。加载图片为纹理加载图片为纹理有两种方式，一种是使用MTLTextureDescriptor创建Texture，需要将图片转为位图的buffer，另一种是使用MetalKit中的MTKTextureLoader，这种方式使用CGImage即可。使用MTLTextureDescriptor创建Texture	@property(nonatomic,strong)id&lt;MTLTexture&gt;texture;	...	NSString*imagePath = [[NSBundle mainBundle]pathForResource:@"container" ofType:@"png"];    UIImage*image = [UIImage imageWithContentsOfFile:imagePath];    CGImageRef cgImageRef = [image CGImage];    GLuint width = (GLuint)CGImageGetWidth(cgImageRef);    GLuint height = (GLuint)CGImageGetHeight(cgImageRef);    CGRect rect = CGRectMake(0, 0, width, height);    void *imageData = malloc(width * height * 4);    CGContextRef context = CGBitmapContextCreate(imageData, width, height, 8, width * 4, CGColorSpaceCreateDeviceRGB(), kCGImageAlphaPremultipliedLast);    CGContextTranslateCTM(context, 0, height);    CGContextScaleCTM(context, 1.0, -1.0);        CGContextDrawImage(context, rect, cgImageRef);    CGContextRelease(context);		...			MTLTextureDescriptor*textureDes = [MTLTextureDescriptor texture2DDescriptorWithPixelFormat:MTLPixelFormatRGBA8Unorm width:width height:height mipmapped:NO];    self.texture = [self.device newTextureWithDescriptor:textureDes];		MTLRegion region = MTLRegionMake2D(0, 0, width, height);    [self.texture replaceRegion:region mipmapLevel:0 withBytes:imageData bytesPerRow:width*4];这里将UIImage转为imageData，然后创建一个纹理描述器，设置宽高、颜色格式。然后device根据描述器创建一个纹理。之后调用replaceRegion：方法将图片数据放入纹理中，其中region参数表示图片的范围。需要注意的是，由于CGImage的坐标中y轴是向上增长的，和UIKit相反，所以CGContextDrawImage的图片会上下颠倒，需要做下翻转。具体可以参考 ios-绘图之图片上下颠倒使用MTKTextureLoader创建Texture    MTKTextureLoader*loader = [[MTKTextureLoader alloc]initWithDevice:self.device];    NSError*error;    self.texture = [loader newTextureWithCGImage:image.CGImage options:@{MTKTextureLoaderOptionSRGB:@(NO)} error:&amp;error];这里使用device创建一个MTKTextureLoader，然后传入图片的CGImage即可。设置texture为renderEncoder的纹理	[renderEncoder setFragmentTexture:self.texture atIndex:0];修改shaderstruct VertexOut{    float4 position [[position]];    float2 textureCoordinate;};vertex VertexOut texture_vertex (    constant float4*vertex_array[[buffer(0)]],    constant float2*textureCoord_array[[buffer(1)]],    unsigned int vid[[vertex_id]]){    VertexOut outputVertices;    outputVertices.position = vertex_array[vid];    outputVertices.textureCoordinate = textureCoord_array[vid];    return outputVertices;}因为渲染纹理需要顶点着色器向片段着色器传顶点坐标和纹理坐标，所以这里定义了一个结构体，用作顶点着色器函数的返回值类型。函数第一个参数表示传入的顶点数据数组，第二参数表示传入的纹理坐标数组。函数的实现就是将顶点和纹理坐标组合成结构体返回。fragment float4 texture_fragment(VertexOut fragmentInput [[stage_in]],                                 texture2d&lt;float&gt; inputTexture [[texture(0)]]) {    constexpr sampler quadSampler;    float4 color = inputTexture.sample(quadSampler, fragmentInput.textureCoordinate);    return color;}这里第一个参数是从顶点着色器返回的，包含顶点和纹理坐标，第二个参数是传入的纹理。函数实现中，定义了一个取样器，用于取得纹理对应坐标的颜色并返回。创建纹理坐标buffer并传入renderEncoder	float textureCoord[] = {        0,0,        1,0,        0,1,        1,1    };    id&lt;MTLBuffer&gt;textureCoordBuffer = [self.device newBufferWithBytes:textureCoord length:sizeof(textureCoord) options:MTLResourceCPUCacheModeDefaultCache];        [renderEncoder setVertexBuffer:textureCoordBuffer offset:0 atIndex:1];这里创建了一个MTLBuffer，并作为序号1的参数传给着色器，对应顶点着色器函数中的textureCoord_array.之后运行即可看到图片纹理。总结在使用MTKTextureLoader时发现它属于MetalKit，和Metal不在同一个framework，MetalKit是为了更方便开发者使用而推出的，并且更加便于和iOS原生的数据交互，例如MTKTextureLoader就可以直接使用CGImage生成纹理，很方便。另外就是Metal的shader语言更加灵活，可以自定义结构体，可以更加灵活的传递各种参数。demo地址]]></content>
      <categories>
        
          <category> Metal </category>
        
      </categories>
      <tags>
        
          <tag> OpenGL </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
          <tag> Metal </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Metal学习笔记01 渲染一个三角形]]></title>
      <url>/metal/2018/10/21/Metal01/</url>
      <content type="text"><![CDATA[Metal是苹果于2014年WWDC中发布的图像处理、通用计算的框架，而在今年的ios12中，OpenGL ES的API也被标记为废弃了，所以作为一个iOS开发者，有必要了解下这个框架。这次还是和学习OpenGL的时候一样，先使用Metal画一个三角形。在iOS下使用Metal创建用于Metal显示内容的组件在Metal中，可以选择使用CAMetalLayer或者MTKView，本文选择使用CAMetalLayer，需要注意的是，在项目的运行设备是模拟器的时候，会提示找不到CAMetalLayer这个类，这是因为Metal取消了模拟器的支持，只能使用64位处理器的真机进行开发和调试。@interface ViewController ()@property(nonatomic,weak)CAMetalLayer * mLayer;@end@implementation ViewController- (void)viewDidLoad {    [super viewDidLoad];    // Do any additional setup after loading the view.       CAMetalLayer*layer = [[CAMetalLayer alloc]init];	//设置layer的像素格式，这里设置为BGRA32    layer.pixelFormat = MTLPixelFormatBGRA8Unorm;	//framebufferOnly根据描述是设置纹理是否只用作显示还是需要做一些采样和计算，一般情况下设置为YES，可以提高性能表现    layer.framebufferOnly = YES;    layer.frame = self.view.bounds;    layer.drawableSize = self.view.bounds.size;}创建MTLDevice	...	@property(nonatomic,strong)id&lt;MTLDevice&gt;device;	...	self.device = MTLCreateSystemDefaultDevice();    layer.device = self.device;设备是一个遵循了MTLDevice协议的类，是对GPU的抽象，许多Metal对象都需要通过设备对象来获取。这里创建了默认的设备，并且设置为CAMetalLayer的设备。创建shader和OpenGL一样，Metal渲染也需要顶点着色器程序和片段着色器程序，使用Metal着色语言编写。在Metal开发中，可以通过xcode创建一个.metal文件，用于编写相应的着色器程序。#include &lt;metal_stdlib&gt;using namespace metal;vertex float4 basic_vertex (    constant packed_float3*vertex_array[[buffer(0)]],    unsigned int vid[[vertex_id]]){            return float4(vertex_array[vid], 1.0);}这里定义了一个顶点着色器程序，vertex关键字用于标记这是一个顶点着色器程序。float4表示这个函数的返回值是一个四维向量，在这里四个值分别表示为x,y,z,w,其中w用于做一些旋转平移缩放时方便计算，一般为1。之后是两个函数的参数，constant修饰第一个参数为常量，是一个三维向量的数组，也就是传入的顶点坐标。中间的[[buffer(0)]]表明是缓存数据，0是索引，索引值用于区分一些时候传入的数据不全是顶点数据时，比如传入的数据包含顶点坐标，颜色和纹理坐标，用索引来获取到正确的数据。第二个参数用与获取当前处理的顶点。函数体中就是返回了一个表示坐标的四维向量。fragment float4 basic_fragment() {    return float4(1.0,0,0,1);}这里定义了一个片段着色器程序，返回一个四维向量表示颜色的rgba，这里固定写为红色。创建MTLLibraryid&lt;MTLLibrary&gt;library = [self.device newDefaultLibrary];创建MTLFunction	id&lt;MTLFunction&gt;vertexFunc = [library newFunctionWithName:@"basic_vertex"];    id&lt;MTLFunction&gt;fragmentFunc = [library newFunctionWithName:@"basic_fragment"];创建一个的管道描述器	MTLRenderPipelineDescriptor*descriptor = [[MTLRenderPipelineDescriptor alloc]init];    descriptor.vertexFunction = vertexFunc;    descriptor.fragmentFunction = fragmentFunc;    descriptor.colorAttachments[0].pixelFormat = MTLPixelFormatBGRA8Unorm;创建一个管道	id&lt;MTLRenderPipelineState&gt; pipelineState = [self.device newRenderPipelineStateWithDescriptor:descriptor error:nil];这样就构建了一个完整的数据处理的管道，接下来需要将数据传入。创建顶点坐标缓冲float vertexArray[] = {        1.0f,  1.0f, 0.0f,        -1.0f, -1.0f, 0.0f,        1.0f, -1.0f, 0.0f    };	id&lt;MTLBuffer&gt;vertexBuffer = [self.device newBufferWithBytes:vertexArray length:sizeof(vertexArray) options:MTLResourceCPUCacheModeDefaultCache];这样就创建了一个顶点缓冲，其中MTLResourceCPUCacheModeDefaultCache表示它可以被GPU、CPU读写，同时也是操作也是有序的。创建命令队列和命令缓冲	id&lt;MTLCommandQueue&gt;commandQueue = [self.device newCommandQueue];	id&lt;MTLCommandBuffer&gt;commandBuffer = [commandQueue commandBuffer];创建一个渲染路径描述器	id&lt;CAMetalDrawable&gt;drawable = [self.mLayer nextDrawable];        MTLRenderPassDescriptor*renderPassDes = [[MTLRenderPassDescriptor alloc]init];        renderPassDes.colorAttachments[0].texture = [drawable texture];    renderPassDes.colorAttachments[0].loadAction = MTLLoadActionClear;    renderPassDes.colorAttachments[0].clearColor = MTLClearColorMake(1.0, 1.0, 1.0, 1.0);先取得layer的drawable，它是一个用于显示的资源，可以被Metal渲染或改写。然后创建了渲染路径描述，描述了一个清屏为白色然后再渲染的操作。创建一个命令编码器	id&lt;MTLRenderCommandEncoder&gt;renderEncoder = [commandBuffer renderCommandEncoderWithDescriptor:renderPassDes];    [renderEncoder setRenderPipelineState:pipelineState];    [renderEncoder setVertexBuffer:vertexBuffer offset:0 atIndex:0];        [renderEncoder drawPrimitives:MTLPrimitiveTypeTriangleStrip vertexStart:0 vertexCount:3];创建一个编码器，并指定之前创建的pipeline和顶点，drawPrimitives:vertexStart:vertexCount,类似glDrawArray函数，不过它应该不是直接绘制，而是编码出一个绘制多边形的命令。提交命令	[commandBuffer presentDrawable:drawable];    [commandBuffer commit];这样就完成了一个三角形的绘制&lt;img src=”https://i.loli.net/2019/06/17/5d075fbe6222099023.jpg” alt=”Metal01.png” title=”Metal01.png” width= 36%/&gt;绘制矩形	float vertexArray[] = {        -1.0f,1.0f,0.0f,        1.0f,  1.0f, 0.0f,        -1.0f, -1.0f, 0.0f,        1.0f, -1.0f, 0.0f    };		...		[renderEncoder drawPrimitives:MTLPrimitiveTypeTriangleStrip vertexStart:0 vertexCount:4];和OpenGL一样，也可以使用4个顶点来绘制一个矩形，修改drawPrimitives：的参数为MTLPrimitiveTypeTriangleStrip，然后顶点顺序为z字形即可。&lt;img src=”https://i.loli.net/2019/06/17/5d0760983cf0b76962.jpg” alt=”Metal01.png” title=”Metal01.png” width= 36%/&gt;总结Metal的主要逻辑和OpenGL类似，都是将顶点数据传给顶点着色器，计算出坐标后在传给片段着色器，由片段着色器计算出像素的颜色。但具体代码的实现上，Metal比OpenGL更符合iOS开发者的习惯，不过Metal感觉渲染的步骤较多，需要好好理解下。demo地址参考：Metal]]></content>
      <categories>
        
          <category> Metal </category>
        
      </categories>
      <tags>
        
          <tag> OpenGL </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
          <tag> Metal </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[记一次静态库导致的审核被拒]]></title>
      <url>/tip/2018/08/23/Tip01/</url>
      <content type="text"><![CDATA[前几天一个使用我们公司iOS sdk的用户跟我说他们app审核被拒了，原因是包含了dlopen(),dlsym()的函数。审核的理由原文：  This code, combined with a remote resource, can facilitate significant changes to your app’s behavior compared to when it was initially reviewed for the App Store. While you may not be using this functionality currently, it has the potential to load private frameworks, private methods, and enable future feature changes. This includes any code which passes arbitrary parameters to dynamic methods such as dlopen(), dlsym(), respondsToSelector:, performSelector:, method_exchangeImplementations(), and running remote scripts in order to change app behavior and/or call SPI, based on the contents of the downloaded script. Even if the remote resource is not intentionally malicious, it could easily be hijacked via a Man In The Middle (MiTM) attack, which can pose a serious security vulnerability to users of your app.查了下dlopen和dlsym的用处，发现是可以动态加载库文件，意味着可以不通过app store审核更新代码，这个肯定是苹果不允许的。不过我记得热更新被禁止是去年的事了，而我们公司sdk的第三方库基本没有动过，这会突然拒绝了一个用户的审核，而其他使用的用户并没有反馈，有点看不懂苹果的审核机制。不过，既然已经被拒了，问题还是要解决，需要找到是哪个库中包含了这些函数。网上查了下，发现可以通过nm命令来查看动态和静态库中的符号。Linux的nm查看动态和静态库中的符号这里使用  nm -u libxxx.a » xxx.txt将sdk中使用的库都查看一遍后，发现OpenSSL的libcrypto.a中确实有这两个函数。也可以看到是在dso_dlfcn文件中，于是去OpenSSL源码中查找这个文件，看看这个函数是在什么情况下编译到静态库中。果然在这个文件中，然后看文件结构，发现这个文件在crypto文件夹中的dso文件夹下，猜测dso可能是crypto库的一个模块。经过网上查资料，发现dso就是一个动态加载的功能模块，在OpenSSL中，使用编译参数no-dso就可以禁用这个模块。于是使用OpenSSL-for-iPhone的编译脚本，在编译时增加参数，重新编译静态库。  CONFIG_OPTIONS=”no-dso” ./build-libssl.sh重新编译后再次查看符号，发现已经没有dlopen和dlsym了。替换了静态库后，重新编译好sdk，提供给用户，过了几天用户反馈审核通过，问题解决。]]></content>
      <categories>
        
          <category> Tip </category>
        
      </categories>
      <tags>
        
          <tag> 笔记 </tag>
        
          <tag> 苹果审核 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[GPUImage源码研究02 研究采集视频滤镜]]></title>
      <url>/gpuimage/2018/06/02/GPUImage02/</url>
      <content type="text"><![CDATA[上一篇笔记这次来研究下GPUImage采集视频滤镜的处理。使用滤镜渲染采集视频- (void)viewDidLoad {    [super viewDidLoad];    // Do any additional setup after loading the view.            GPUImageVideoCamera *videoCamera = [[GPUImageVideoCamera alloc] initWithSessionPreset:AVCaptureSessionPreset640x480 cameraPosition:AVCaptureDevicePositionFront];    videoCamera.outputImageOrientation = UIInterfaceOrientationPortrait;        GPUImageSketchFilter *customFilter = [[GPUImageSketchFilter alloc] init];    GPUImageView *filteredVideoView = [[GPUImageView alloc] initWithFrame:self.view.bounds];    [self.view addSubview:filteredVideoView];    // Add the view somewhere so it's visible        [videoCamera addTarget:customFilter];    [customFilter addTarget:filteredVideoView];        [videoCamera startCameraCapture];    self.videoCamera = videoCamera;}这里使用GPUImageVideoCamera来作为输入源，然后添加一个滤镜，再添加一个GPUImageView输出显示。使用GPUImageVideoCamera采集视频数据初始化方法- (id)initWithSessionPreset:(NSString *)sessionPreset cameraPosition:(AVCaptureDevicePosition)cameraPosition; {	if (!(self = [super init]))    {		return nil;    }       	...	// Create the capture session	_captureSession = [[AVCaptureSession alloc] init];	    [_captureSession beginConfiguration];    	// Add the video input		NSError *error = nil;	videoInput = [[AVCaptureDeviceInput alloc] initWithDevice:_inputCamera error:&amp;error];	if ([_captureSession canAddInput:videoInput]) 	{		[_captureSession addInput:videoInput];	}		// Add the video frame output		videoOutput = [[AVCaptureVideoDataOutput alloc] init];	[videoOutput setAlwaysDiscardsLateVideoFrames:NO];	...}首先创建了AVCaptureSession以及相关的输入输出。- (id)initWithSessionPreset:(NSString *)sessionPreset cameraPosition:(AVCaptureDevicePosition)cameraPosition; {   	...//    if (captureAsYUV &amp;&amp; [GPUImageContext deviceSupportsRedTextures])    if (captureAsYUV &amp;&amp; [GPUImageContext supportsFastTextureUpload])    {        BOOL supportsFullYUVRange = NO;        NSArray *supportedPixelFormats = videoOutput.availableVideoCVPixelFormatTypes;        for (NSNumber *currentPixelFormat in supportedPixelFormats)        {            if ([currentPixelFormat intValue] == kCVPixelFormatType_420YpCbCr8BiPlanarFullRange)            {                supportsFullYUVRange = YES;            }        }                if (supportsFullYUVRange)        {            [videoOutput setVideoSettings:[NSDictionary dictionaryWithObject:[NSNumber numberWithInt:kCVPixelFormatType_420YpCbCr8BiPlanarFullRange] forKey:(id)kCVPixelBufferPixelFormatTypeKey]];            isFullYUVRange = YES;        }        else        {            [videoOutput setVideoSettings:[NSDictionary dictionaryWithObject:[NSNumber numberWithInt:kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange] forKey:(id)kCVPixelBufferPixelFormatTypeKey]];            isFullYUVRange = NO;        }    }    else    {        [videoOutput setVideoSettings:[NSDictionary dictionaryWithObject:[NSNumber numberWithInt:kCVPixelFormatType_32BGRA] forKey:(id)kCVPixelBufferPixelFormatTypeKey]];    }	... }然后设置采集输出的数据类型，默认是kCVPixelFormatType_420YpCbCr8BiPlanarFullRange的。 - (id)initWithSessionPreset:(NSString *)sessionPreset cameraPosition:(AVCaptureDevicePosition)cameraPosition; {   	...    runSynchronouslyOnVideoProcessingQueue(^{                if (captureAsYUV)        {            [GPUImageContext useImageProcessingContext];            //            if ([GPUImageContext deviceSupportsRedTextures])            //            {            //                yuvConversionProgram = [[GPUImageContext sharedImageProcessingContext] programForVertexShaderString:kGPUImageVertexShaderString fragmentShaderString:kGPUImageYUVVideoRangeConversionForRGFragmentShaderString];            //            }            //            else            //            {            if (isFullYUVRange)            {                yuvConversionProgram = [[GPUImageContext sharedImageProcessingContext] programForVertexShaderString:kGPUImageVertexShaderString fragmentShaderString:kGPUImageYUVFullRangeConversionForLAFragmentShaderString];            }            else            {                yuvConversionProgram = [[GPUImageContext sharedImageProcessingContext] programForVertexShaderString:kGPUImageVertexShaderString fragmentShaderString:kGPUImageYUVVideoRangeConversionForLAFragmentShaderString];            }            //            }                        if (!yuvConversionProgram.initialized)            {                [yuvConversionProgram addAttribute:@"position"];                [yuvConversionProgram addAttribute:@"inputTextureCoordinate"];                                if (![yuvConversionProgram link])                {                    NSString *progLog = [yuvConversionProgram programLog];                    NSLog(@"Program link log: %@", progLog);                    NSString *fragLog = [yuvConversionProgram fragmentShaderLog];                    NSLog(@"Fragment shader compile log: %@", fragLog);                    NSString *vertLog = [yuvConversionProgram vertexShaderLog];                    NSLog(@"Vertex shader compile log: %@", vertLog);                    yuvConversionProgram = nil;                    NSAssert(NO, @"Filter shader link failed");                }            }                        yuvConversionPositionAttribute = [yuvConversionProgram attributeIndex:@"position"];            yuvConversionTextureCoordinateAttribute = [yuvConversionProgram attributeIndex:@"inputTextureCoordinate"];            yuvConversionLuminanceTextureUniform = [yuvConversionProgram uniformIndex:@"luminanceTexture"];            yuvConversionChrominanceTextureUniform = [yuvConversionProgram uniformIndex:@"chrominanceTexture"];            yuvConversionMatrixUniform = [yuvConversionProgram uniformIndex:@"colorConversionMatrix"];                        [GPUImageContext setActiveShaderProgram:yuvConversionProgram];                        glEnableVertexAttribArray(yuvConversionPositionAttribute);            glEnableVertexAttribArray(yuvConversionTextureCoordinateAttribute);        }    });		...}接下来判断是否采集NV12数据，如果采集的话，会创建一个OpenGL的program，用于后续将采集到的NV12数据转换为RGBA。        [videoOutput setSampleBufferDelegate:self queue:cameraProcessingQueue];	if ([_captureSession canAddOutput:videoOutput])	{		[_captureSession addOutput:videoOutput];	}	else	{		NSLog(@"Couldn't add video output");        return nil;	}    	_captureSessionPreset = sessionPreset;    [_captureSession setSessionPreset:_captureSessionPreset];// This will let you get 60 FPS video from the 720p preset on an iPhone 4S, but only that device and that preset//    AVCaptureConnection *conn = [videoOutput connectionWithMediaType:AVMediaTypeVideo];//    //    if (conn.supportsVideoMinFrameDuration)//        conn.videoMinFrameDuration = CMTimeMake(1,60);//    if (conn.supportsVideoMaxFrameDuration)//        conn.videoMaxFrameDuration = CMTimeMake(1,60);        [_captureSession commitConfiguration];    	return self;}最后则是添加为videoDataOutput的代理，用于接受和处理采集的视频数据。采集数据后的操作接下来看在videoDataOutput的代理方法中的代码。- (void)captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{    if (!self.captureSession.isRunning)    {        return;    }    else if (captureOutput == audioOutput)    {        [self processAudioSampleBuffer:sampleBuffer];    }    else    {        if (dispatch_semaphore_wait(frameRenderingSemaphore, DISPATCH_TIME_NOW) != 0)        {            return;        }                CFRetain(sampleBuffer);        runAsynchronouslyOnVideoProcessingQueue(^{            //Feature Detection Hook.            if (self.delegate)            {                [self.delegate willOutputSampleBuffer:sampleBuffer];            }                        [self processVideoSampleBuffer:sampleBuffer];                        CFRelease(sampleBuffer);            dispatch_semaphore_signal(frameRenderingSemaphore);        });    }}可以看到主要就是判断sampleBuffer是音频帧还是视频帧，然后执行了一个代理方法用于给使用者获取到滤镜处理之前的数据。之后就是将视频帧作为参数执行processVideoSampleBuffer：方法。processVideoSampleBuffer方法- (void)processVideoSampleBuffer:(CMSampleBufferRef)sampleBuffer;{    if (capturePaused)    {        return;    }        CFAbsoluteTime startTime = CFAbsoluteTimeGetCurrent();    CVImageBufferRef cameraFrame = CMSampleBufferGetImageBuffer(sampleBuffer);    int bufferWidth = (int) CVPixelBufferGetWidth(cameraFrame);    int bufferHeight = (int) CVPixelBufferGetHeight(cameraFrame);    CFTypeRef colorAttachments = CVBufferGetAttachment(cameraFrame, kCVImageBufferYCbCrMatrixKey, NULL);    if (colorAttachments != NULL)    {        if(CFStringCompare(colorAttachments, kCVImageBufferYCbCrMatrix_ITU_R_601_4, 0) == kCFCompareEqualTo)        {            if (isFullYUVRange)            {                _preferredConversion = kColorConversion601FullRange;            }            else            {                _preferredConversion = kColorConversion601;            }        }        else        {            _preferredConversion = kColorConversion709;        }    }    else    {        if (isFullYUVRange)        {            _preferredConversion = kColorConversion601FullRange;        }        else        {            _preferredConversion = kColorConversion601;        }    }	...}首先是获取一些视频帧的信息，最主要的是确定颜色格式，根据颜色格式设置一个用于OpenGL 处理颜色转换的矩阵。- (void)processVideoSampleBuffer:(CMSampleBufferRef)sampleBuffer;{	...	CMTime currentTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer);    [GPUImageContext useImageProcessingContext];    if ([GPUImageContext supportsFastTextureUpload] &amp;&amp; captureAsYUV)    {        CVOpenGLESTextureRef luminanceTextureRef = NULL;        CVOpenGLESTextureRef chrominanceTextureRef = NULL;        if (CVPixelBufferGetPlaneCount(cameraFrame) &gt; 0) // Check for YUV planar inputs to do RGB conversion        {            CVPixelBufferLockBaseAddress(cameraFrame, 0);                        if ( (imageBufferWidth != bufferWidth) &amp;&amp; (imageBufferHeight != bufferHeight) )            {                imageBufferWidth = bufferWidth;                imageBufferHeight = bufferHeight;            }                        CVReturn err;            // Y-plane            glActiveTexture(GL_TEXTURE4);            if ([GPUImageContext deviceSupportsRedTextures])            {                err = CVOpenGLESTextureCacheCreateTextureFromImage(kCFAllocatorDefault, [[GPUImageContext sharedImageProcessingContext] coreVideoTextureCache], cameraFrame, NULL, GL_TEXTURE_2D, GL_LUMINANCE, bufferWidth, bufferHeight, GL_LUMINANCE, GL_UNSIGNED_BYTE, 0, &amp;luminanceTextureRef);            }            else            {                err = CVOpenGLESTextureCacheCreateTextureFromImage(kCFAllocatorDefault, [[GPUImageContext sharedImageProcessingContext] coreVideoTextureCache], cameraFrame, NULL, GL_TEXTURE_2D, GL_LUMINANCE, bufferWidth, bufferHeight, GL_LUMINANCE, GL_UNSIGNED_BYTE, 0, &amp;luminanceTextureRef);            }            if (err)            {                NSLog(@"Error at CVOpenGLESTextureCacheCreateTextureFromImage %d", err);            }                        luminanceTexture = CVOpenGLESTextureGetName(luminanceTextureRef);            glBindTexture(GL_TEXTURE_2D, luminanceTexture);            glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);            glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);                        // UV-plane            glActiveTexture(GL_TEXTURE5);            if ([GPUImageContext deviceSupportsRedTextures])            {//                err = CVOpenGLESTextureCacheCreateTextureFromImage(kCFAllocatorDefault, coreVideoTextureCache, cameraFrame, NULL, GL_TEXTURE_2D, GL_RG_EXT, bufferWidth/2, bufferHeight/2, GL_RG_EXT, GL_UNSIGNED_BYTE, 1, &amp;chrominanceTextureRef);                err = CVOpenGLESTextureCacheCreateTextureFromImage(kCFAllocatorDefault, [[GPUImageContext sharedImageProcessingContext] coreVideoTextureCache], cameraFrame, NULL, GL_TEXTURE_2D, GL_LUMINANCE_ALPHA, bufferWidth/2, bufferHeight/2, GL_LUMINANCE_ALPHA, GL_UNSIGNED_BYTE, 1, &amp;chrominanceTextureRef);            }            else            {                err = CVOpenGLESTextureCacheCreateTextureFromImage(kCFAllocatorDefault, [[GPUImageContext sharedImageProcessingContext] coreVideoTextureCache], cameraFrame, NULL, GL_TEXTURE_2D, GL_LUMINANCE_ALPHA, bufferWidth/2, bufferHeight/2, GL_LUMINANCE_ALPHA, GL_UNSIGNED_BYTE, 1, &amp;chrominanceTextureRef);            }            if (err)            {                NSLog(@"Error at CVOpenGLESTextureCacheCreateTextureFromImage %d", err);            }                        chrominanceTexture = CVOpenGLESTextureGetName(chrominanceTextureRef);            glBindTexture(GL_TEXTURE_2D, chrominanceTexture);            glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);            glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);            //            if (!allTargetsWantMonochromeData)//            {                [self convertYUVToRGBOutput];//            }            int rotatedImageBufferWidth = bufferWidth, rotatedImageBufferHeight = bufferHeight;                        if (GPUImageRotationSwapsWidthAndHeight(internalRotation))            {                rotatedImageBufferWidth = bufferHeight;                rotatedImageBufferHeight = bufferWidth;            }                        [self updateTargetsForVideoCameraUsingCacheTextureAtWidth:rotatedImageBufferWidth height:rotatedImageBufferHeight time:currentTime];                        CVPixelBufferUnlockBaseAddress(cameraFrame, 0);            CFRelease(luminanceTextureRef);            CFRelease(chrominanceTextureRef);        }				...}接下来就是处理视频原始数据了。首先判断了视频的颜色格式，如果属于yuv格式的，则创建两个CVOpenGLESTextureRef用于接受y分量和uv分量的纹理。在CVOpenGLESTextureRef的定义出可以看出CVOpenGLESTextureRef实际上就是CVImageBufferRef的别名。  @typedef	CVOpenGLESTextureRef  @abstract   OpenGLES texture based image buffer  typedef CVImageBufferRef CVOpenGLESTextureRef;接下来就是调用CVOpenGLESTextureCacheCreateTextureFromImage这个方法将y分量和uv分量分别写入两个纹理并与OpenGL的纹理绑定。然后就是调用方法convertYUVToRGBOutput来将yuv数据转换成RGB数据并放入帧缓冲中，并且作为纹理存放在framebuffer的_texture中。然后调用updateTargetsForVideoCameraUsingCacheTextureAtWidth方法，将帧缓冲交给响应链的下一个对象处理。- (void)processVideoSampleBuffer:(CMSampleBufferRef)sampleBuffer;{	...	CMTime currentTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer);    [GPUImageContext useImageProcessingContext];    if ([GPUImageContext supportsFastTextureUpload] &amp;&amp; captureAsYUV)	{		...    }    else    {        CVPixelBufferLockBaseAddress(cameraFrame, 0);                int bytesPerRow = (int) CVPixelBufferGetBytesPerRow(cameraFrame);        outputFramebuffer = [[GPUImageContext sharedFramebufferCache] fetchFramebufferForSize:CGSizeMake(bytesPerRow / 4, bufferHeight) onlyTexture:YES];        [outputFramebuffer activateFramebuffer];        glBindTexture(GL_TEXTURE_2D, [outputFramebuffer texture]);                // Using BGRA extension to pull in video frame data directly        // The use of bytesPerRow / 4 accounts for a display glitch present in preview video frames when using the photo preset on the camera        glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, bytesPerRow / 4, bufferHeight, 0, GL_BGRA, GL_UNSIGNED_BYTE, CVPixelBufferGetBaseAddress(cameraFrame));                [self updateTargetsForVideoCameraUsingCacheTextureAtWidth:bytesPerRow / 4 height:bufferHeight time:currentTime];                CVPixelBufferUnlockBaseAddress(cameraFrame, 0);              ...    }  }视频数据是RGB的情况的话，就直接将数据写入outputFramebuffer的纹理中，然后执行响应链后续的处理。而响应链的处理，就是将outputFramebuffer设置为下一对象的inputFramebuffer，然后执行newFrameReadyAtTime:atIndex: 方法做下一步处理，这个方法具体的处理在上一篇笔记中有过分析。分析GPUImageView在响应链中，经过filter的处理后，GPUImageView作为处理的终点，将画面渲染出来。- (id)initWithFrame:(CGRect)frame{    if (!(self = [super initWithFrame:frame]))    {		return nil;    }        [self commonInit];        return self;}- (void)commonInit;{    // Set scaling to account for Retina display	    if ([self respondsToSelector:@selector(setContentScaleFactor:)])    {        self.contentScaleFactor = [[UIScreen mainScreen] scale];    }    inputRotation = kGPUImageNoRotation;    self.opaque = YES;    self.hidden = NO;    CAEAGLLayer *eaglLayer = (CAEAGLLayer *)self.layer;    eaglLayer.opaque = YES;    eaglLayer.drawableProperties = [NSDictionary dictionaryWithObjectsAndKeys:[NSNumber numberWithBool:NO], kEAGLDrawablePropertyRetainedBacking, kEAGLColorFormatRGBA8, kEAGLDrawablePropertyColorFormat, nil];    self.enabled = YES;        runSynchronouslyOnVideoProcessingQueue(^{        [GPUImageContext useImageProcessingContext];                displayProgram = [[GPUImageContext sharedImageProcessingContext] programForVertexShaderString:kGPUImageVertexShaderString fragmentShaderString:kGPUImagePassthroughFragmentShaderString];        if (!displayProgram.initialized)        {            [displayProgram addAttribute:@"position"];            [displayProgram addAttribute:@"inputTextureCoordinate"];                        if (![displayProgram link])            {                NSString *progLog = [displayProgram programLog];                NSLog(@"Program link log: %@", progLog);                NSString *fragLog = [displayProgram fragmentShaderLog];                NSLog(@"Fragment shader compile log: %@", fragLog);                NSString *vertLog = [displayProgram vertexShaderLog];                NSLog(@"Vertex shader compile log: %@", vertLog);                displayProgram = nil;                NSAssert(NO, @"Filter shader link failed");            }        }                displayPositionAttribute = [displayProgram attributeIndex:@"position"];        displayTextureCoordinateAttribute = [displayProgram attributeIndex:@"inputTextureCoordinate"];        displayInputTextureUniform = [displayProgram uniformIndex:@"inputImageTexture"]; // This does assume a name of "inputTexture" for the fragment shader        [GPUImageContext setActiveShaderProgram:displayProgram];        glEnableVertexAttribArray(displayPositionAttribute);        glEnableVertexAttribArray(displayTextureCoordinateAttribute);                [self setBackgroundColorRed:0.0 green:0.0 blue:0.0 alpha:1.0];        _fillMode = kGPUImageFillModePreserveAspectRatio;        [self createDisplayFramebuffer];    });}可以看到，GPUImageView，基本上就是使用OpenGL实现了一个用于渲染RGBA数据的view。在初始化时，首先将自身的layer设置为CAEAGLLayer，然后创建一个用于渲染显示的program,链接并使用它。之后再配置一些输入参数。最后使用createDisplayFramebuffer方法创建一个用于显示的帧缓冲。由于GPUImageView也实现了GPUImageInput协议，因此也实现了newFrameReadyAtTime: atIndex:方法，用于处理接收到的视频数据。- (void)newFrameReadyAtTime:(CMTime)frameTime atIndex:(NSInteger)textureIndex;{    runSynchronouslyOnVideoProcessingQueue(^{        [GPUImageContext setActiveShaderProgram:displayProgram];        [self setDisplayFramebuffer];                glClearColor(backgroundColorRed, backgroundColorGreen, backgroundColorBlue, backgroundColorAlpha);        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);                glActiveTexture(GL_TEXTURE4);        glBindTexture(GL_TEXTURE_2D, [inputFramebufferForDisplay texture]);        glUniform1i(displayInputTextureUniform, 4);                glVertexAttribPointer(displayPositionAttribute, 2, GL_FLOAT, 0, 0, imageVertices);        glVertexAttribPointer(displayTextureCoordinateAttribute, 2, GL_FLOAT, 0, 0, [GPUImageView textureCoordinatesForRotation:inputRotation]);                glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);                [self presentFramebuffer];        [inputFramebufferForDisplay unlock];        inputFramebufferForDisplay = nil;    });}这里就是实现了OpenGL的渲染过程，首先在setDisplayFramebuffer方法中绑定用于显示的帧缓冲，然后将inputFramebufferForDisplay的纹理绑定并作为参数传入片段着色器，然后传入顶点和纹理坐标，调用glDrawArrays进行绘制，最后调用presentFramebuffer方法，使用renderbuffer渲染画面。补充，获取滤镜处理后的数据- (void)viewDidLoad {    [super viewDidLoad];    // Do any additional setup after loading the view.            GPUImageVideoCamera *videoCamera = [[GPUImageVideoCamera alloc] initWithSessionPreset:AVCaptureSessionPreset640x480 cameraPosition:AVCaptureDevicePositionFront];    videoCamera.outputImageOrientation = UIInterfaceOrientationPortrait;        GPUImageSketchFilter *customFilter = [[GPUImageSketchFilter alloc] init];    GPUImageView *filteredVideoView = [[GPUImageView alloc] initWithFrame:self.view.bounds];    [self.view addSubview:filteredVideoView];    // Add the view somewhere so it's visible        [videoCamera addTarget:customFilter];//    [customFilter addTarget:filteredVideoView];        [videoCamera startCameraCapture];    self.videoCamera = videoCamera;        GPUImageRawDataOutput*output= [[GPUImageRawDataOutput alloc]initWithImageSize:CGSizeMake(480, 640) resultsInBGRAFormat:YES];    __weak GPUImageRawDataOutput *weakOutput = output;        __weak typeof(self) weakSelf = self;    output.newFrameAvailableBlock = ^{        [weakOutput lockFramebufferForReading];        GLubyte *data  = [weakOutput rawBytesForImage];		//对数据后续的处理，发送等。。。				//这里用一个渲染view去渲染BGRA数据        [weakSelf.displayView displayWithRGBBuffer:data width:480 height:640];        [weakOutput unlockFramebufferAfterReading];    };        self.output = output;    [customFilter addTarget:output];        LYRSampleBufferDisplayView*displayView = [[LYRSampleBufferDisplayView alloc]initWithFrame:self.view.bounds];    [self.view addSubview:displayView];    self.displayView = displayView;}最开始调用的代码，只是将采集视频经过滤镜处理后，渲染显示在GPUImageView上，而实际应用时，一般需要将处理后的数据发送出去，而不是简单地在本地显示。获取处理后的数据，就需要用到GPUImageRawDataOutput。GPUImageRawDataOutput也是一个实现GPUImageInput协议的类，也可以作为响应链中的target，然后给它的newFrameAvailableBlock赋值，在block中通过rawBytesForImage方法取得数据。- (void)newFrameReadyAtTime:(CMTime)frameTime atIndex:(NSInteger)textureIndex;{    hasReadFromTheCurrentFrame = NO;        if (_newFrameAvailableBlock != NULL)    {        _newFrameAvailableBlock();    }}可以看到GPUImageRawDataOutput在newFrameReadyAtTime:atIndex:方法中执行_newFrameAvailableBlock这个block，用于每次取得渲染好的数据后回调。总结GPUImage的视频实时滤镜处理实际上就是一个响应链由GPUImageVideoCamera为起点，经过一系列filter的处理后，由GPUImageView或GPUImageRawDataOutput作为链条的终点。并且也可以看到GPUImage中有些对OpenGL的调用使用了CoreVideo框架的接口，后续也可以研究一下。]]></content>
      <categories>
        
          <category> GPUImage </category>
        
      </categories>
      <tags>
        
          <tag> OpenGL </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
          <tag> GPUImage </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[GPUImage源码研究01 研究图片滤镜过程]]></title>
      <url>/gpuimage/2018/05/01/GPUImage01/</url>
      <content type="text"><![CDATA[之前在一家短视频应用的小创业公司待过，当时领导丢给我几个shader文件和图片，然后叫我用GPUImage把这几个滤镜弄出来，当时看那些shader和GPUImage是一脸懵逼，硬着头皮弄，后来也弄出来了，现在想想当时也没有顺便研究下OpenGL和GPUImage，感觉有点浪费，现在想起来了，之前对OpenGL也有些了解了，就来看看GPUImage的源码吧。使用滤镜渲染一张图片这里直接使用GPUImage github主页提供的图片滤镜代码示例,取得滤镜处理过后的图片后，放到UIImageView上显示。@interface ViewController ()@property (weak, nonatomic) IBOutlet UIImageView *filteredImageView;@end@implementation ViewController- (void)viewDidLoad {    [super viewDidLoad];    // Do any additional setup after loading the view.        UIImage *inputImage = [UIImage imageWithContentsOfFile:[[NSBundle mainBundle]pathForResource:@"container" ofType:@"png"]];        GPUImagePicture *stillImageSource = [[GPUImagePicture alloc] initWithImage:inputImage];    GPUImageSepiaFilter *stillImageFilter = [[GPUImageSepiaFilter alloc] init];        [stillImageSource addTarget:stillImageFilter];    [stillImageFilter useNextFrameForImageCapture];    [stillImageSource processImage];        UIImage *currentFilteredVideoFrame = [stillImageFilter imageFromCurrentFramebuffer];        self.filteredImageView.image = currentFilteredVideoFrame;}@end使用GPUImagePicture加载图片首先使用图片创建了一个GPUImagePicture对象。获取图片位图数据- (id)initWithImage:(UIImage *)newImageSource;{    if (!(self = [self initWithImage:newImageSource smoothlyScaleOutput:NO]))    {		return nil;    }        return self;}- (id)initWithImage:(UIImage *)newImageSource smoothlyScaleOutput:(BOOL)smoothlyScaleOutput;{    return [self initWithCGImage:[newImageSource CGImage] smoothlyScaleOutput:smoothlyScaleOutput];}- (id)initWithCGImage:(CGImageRef)newImageSource smoothlyScaleOutput:(BOOL)smoothlyScaleOutput removePremultiplication:(BOOL)removePremultiplication{	...}进入init方法,可以看到最终将传入的UIImage的CGImageRef取出，然后调用方法- (id)initWithCGImage:(CGImageRef)newImageSource smoothlyScaleOutput:(BOOL)smoothlyScaleOutput removePremultiplication:(BOOL)removePremultiplication;后两个参数一个是平滑缩放，另一个字面意思是删除复制前，暂时不明，不过默认都是NO。- (id)initWithCGImage:(CGImageRef)newImageSource smoothlyScaleOutput:(BOOL)smoothlyScaleOutput removePremultiplication:(BOOL)removePremultiplication;{    if (!(self = [super init]))    {		return nil;    }        hasProcessedImage = NO;    self.shouldSmoothlyScaleOutput = smoothlyScaleOutput;    imageUpdateSemaphore = dispatch_semaphore_create(0);    dispatch_semaphore_signal(imageUpdateSemaphore);    // TODO: Dispatch this whole thing asynchronously to move image loading off main thread    CGFloat widthOfImage = CGImageGetWidth(newImageSource);    CGFloat heightOfImage = CGImageGetHeight(newImageSource);    // If passed an empty image reference, CGContextDrawImage will fail in future versions of the SDK.    NSAssert( widthOfImage &gt; 0 &amp;&amp; heightOfImage &gt; 0, @"Passed image must not be empty - it should be at least 1px tall and wide");        pixelSizeOfImage = CGSizeMake(widthOfImage, heightOfImage);    CGSize pixelSizeToUseForTexture = pixelSizeOfImage;        BOOL shouldRedrawUsingCoreGraphics = NO;        // For now, deal with images larger than the maximum texture size by resizing to be within that limit    CGSize scaledImageSizeToFitOnGPU = [GPUImageContext sizeThatFitsWithinATextureForSize:pixelSizeOfImage];    ...}首先初始化对象，创建一个信号量，然后获取宽高，并使用sizeThatFitsWithinATextureForSize来判断是否宽高是否有超出纹理最大限制，如果没有，就返回原来的宽高，如果有，则按比例缩小到纹理最大限制之内。- (id)initWithCGImage:(CGImageRef)newImageSource smoothlyScaleOutput:(BOOL)smoothlyScaleOutput removePremultiplication:(BOOL)removePremultiplication{	...	GLubyte *imageData = NULL;    CFDataRef dataFromImageDataProvider = NULL;    GLenum format = GL_BGRA;    BOOL isLitteEndian = YES;    BOOL alphaFirst = NO;            ...        CGBitmapInfo bitmapInfo = CGImageGetBitmapInfo(newImageSource);    ...        if (byteOrderInfo == kCGBitmapByteOrder32Little) {                    /* Little endian, for alpha-first we can use this bitmap directly in GL */                    CGImageAlphaInfo alphaInfo = bitmapInfo &amp; kCGBitmapAlphaInfoMask;                    if (alphaInfo != kCGImageAlphaPremultipliedFirst &amp;&amp; alphaInfo != kCGImageAlphaFirst &amp;&amp;                        alphaInfo != kCGImageAlphaNoneSkipFirst) {                        shouldRedrawUsingCoreGraphics = YES;                    }                } else if (byteOrderInfo == kCGBitmapByteOrderDefault || byteOrderInfo == kCGBitmapByteOrder32Big) {					isLitteEndian = NO;                    /* Big endian, for alpha-last we can use this bitmap directly in GL */                    CGImageAlphaInfo alphaInfo = bitmapInfo &amp; kCGBitmapAlphaInfoMask;                    if (alphaInfo != kCGImageAlphaPremultipliedLast &amp;&amp; alphaInfo != kCGImageAlphaLast &amp;&amp;                        alphaInfo != kCGImageAlphaNoneSkipLast) {                        shouldRedrawUsingCoreGraphics = YES;                    } else {                        /* Can access directly using GL_RGBA pixel format */						premultiplied = alphaInfo == kCGImageAlphaPremultipliedLast || alphaInfo == kCGImageAlphaPremultipliedLast;						alphaFirst = alphaInfo == kCGImageAlphaFirst || alphaInfo == kCGImageAlphaPremultipliedFirst;						format = GL_RGBA;                    }                }                  ...}  然后会取出传入图片的bitmapInfo，然后做一系列判断，确认图片格式是RGBA还是ARGB。- (id)initWithCGImage:(CGImageRef)newImageSource smoothlyScaleOutput:(BOOL)smoothlyScaleOutput removePremultiplication:(BOOL)removePremultiplication{	...	dataFromImageDataProvider = CGDataProviderCopyData(CGImageGetDataProvider(newImageSource));    imageData = (GLubyte *)CFDataGetBytePtr(dataFromImageDataProvider); 	...       }然后将图片的数据拷贝出来。（之中省略一些判断）这样就获得了位图数据。将图片数据写入纹理缓冲runSynchronouslyOnVideoProcessingQueue(^{        [GPUImageContext useImageProcessingContext];                outputFramebuffer = [[GPUImageContext sharedFramebufferCache] fetchFramebufferForSize:pixelSizeToUseForTexture onlyTexture:YES];        [outputFramebuffer disableReferenceCounting];        glBindTexture(GL_TEXTURE_2D, [outputFramebuffer texture]);        if (self.shouldSmoothlyScaleOutput)        {            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR_MIPMAP_LINEAR);        }        // no need to use self.outputTextureOptions here since pictures need this texture formats and type        glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, (int)pixelSizeToUseForTexture.width, (int)pixelSizeToUseForTexture.height, 0, format, GL_UNSIGNED_BYTE, imageData);                if (self.shouldSmoothlyScaleOutput)        {            glGenerateMipmap(GL_TEXTURE_2D);        }        glBindTexture(GL_TEXTURE_2D, 0);    });这里使用了一个GPUImage创建的串行队列，然后同步的调用后续的代码。首先设置上下文，GPUImageContext是对EAGLContext的一个封装，创建了一个GPUImage使用的唯一的EAGLContext，然后设置为当前线程的context。然后从framebuffer的缓冲区获取一个可用的framebuffer，这个缓冲区使用类似字典的结构，根据需要的framebuffer的大小和纹理的一些设置选项进行hash计算，然后以hash值为key来获取可用的framebuffer。GPUImageFramebuffer是对OpenGL纹理和帧缓冲的封装，之后就是将图片数据写入这个纹理缓冲。picture对象持有这个framebuffer，这样一个GPUImagePicture的初始化过程就完成了。创建一个GPUImageFilter对象@implementation GPUImageSepiaFilter- (id)init;{    if (!(self = [super init]))    {		return nil;    }        self.intensity = 1.0;    self.colorMatrix = (GPUMatrix4x4){        {0.3588, 0.7044, 0.1368, 0.0},        {0.2990, 0.5870, 0.1140, 0.0},        {0.2392, 0.4696, 0.0912 ,0.0},        {0,0,0,1.0},    };    return self;}@end这里使用了GPUImageSepiaFilter这个滤镜，它继承自GPUImageColorMatrixFilter，它的init方法就是调用了父类的init方法之后，再赋值了两个参数。- (id)init;{    if (!(self = [super initWithFragmentShaderFromString:kGPUImageColorMatrixFragmentShaderString]))    {        return nil;    }        colorMatrixUniform = [filterProgram uniformIndex:@"colorMatrix"];    intensityUniform = [filterProgram uniformIndex:@"intensity"];        self.intensity = 1.f;    self.colorMatrix = (GPUMatrix4x4){        {1.f, 0.f, 0.f, 0.f},        {0.f, 1.f, 0.f, 0.f},        {0.f, 0.f, 1.f, 0.f},        {0.f, 0.f, 0.f, 1.f}    };        return self;}再看到GPUImageColorMatrixFilter的init方法，实际上调用了GPUImageColorMatrixFilter的父类GPUImageFilter的initWithFragmentShaderFromString:方法，参数传入一个片段着色器的shader字符串。- (id)initWithFragmentShaderFromString:(NSString *)fragmentShaderString;{    if (!(self = [self initWithVertexShaderFromString:kGPUImageVertexShaderString fragmentShaderFromString:fragmentShaderString]))    {		return nil;    }        return self;}再看GPUImageFilter的initWithFragmentShaderFromString方法，实际上就是使用了一个默认的kGPUImageVertexShaderString顶点着色器shader，和传入的片段着色器shader，来创建一个filter对象。- (id)initWithVertexShaderFromString:(NSString *)vertexShaderString fragmentShaderFromString:(NSString *)fragmentShaderString;{    ...    runSynchronouslyOnVideoProcessingQueue(^{        [GPUImageContext useImageProcessingContext];        filterProgram = [[GPUImageContext sharedImageProcessingContext] programForVertexShaderString:vertexShaderString fragmentShaderString:fragmentShaderString];               ...                filterPositionAttribute = [filterProgram attributeIndex:@"position"];        filterTextureCoordinateAttribute = [filterProgram attributeIndex:@"inputTextureCoordinate"];        filterInputTextureUniform = [filterProgram uniformIndex:@"inputImageTexture"]; // This does assume a name of "inputImageTexture" for the fragment shader                [GPUImageContext setActiveShaderProgram:filterProgram];                glEnableVertexAttribArray(filterPositionAttribute);        glEnableVertexAttribArray(filterTextureCoordinateAttribute);        });        return self;}这里可以看到initWithVertexShaderFromString:fragmentShaderFromString:方法使用全局的GPUImageContext，创建了一个GLProgram，GLProgram是对OpenGL的program的一个封装，它内部实现了顶点着色器和片段着色器的编译、链接，以及program的运行。GPUImageContext中也包含了一个对GLProgram的缓存，使用顶点着色器shader和片段着色器shader字符串拼接的字符串为key。获取了GLProgram后，就获取到顶点着色器和片段着色器的输入参数。NSString *const kGPUImageVertexShaderString = SHADER_STRING( attribute vec4 position; attribute vec4 inputTextureCoordinate;  varying vec2 textureCoordinate;  void main() {     gl_Position = position;     textureCoordinate = inputTextureCoordinate.xy; } );接下来看下顶点着色器程序，就是有两个输入，输入顶点和纹理坐标，然后将gl_Position和textureCoordinate传给片段着色器，了解OpenGL后，就会发现渲染视频图片之类的，顶点着色器基本都是这么写。NSString *const kGPUImageColorMatrixFragmentShaderString = SHADER_STRING( varying highp vec2 textureCoordinate;  uniform sampler2D inputImageTexture;  uniform lowp mat4 colorMatrix; uniform lowp float intensity;  void main() {     lowp vec4 textureColor = texture2D(inputImageTexture, textureCoordinate);     lowp vec4 outputColor = textureColor * colorMatrix;          gl_FragColor = (intensity * outputColor) + ((1.0 - intensity) * textureColor); });接下来看GPUImageColorMatrixFilter的片段着色器程序，可以看到除了传入的纹理坐标和纹理输入外，还有两个参数，分别是一个颜色矩阵，和一个表达变化强度的参数，然后实际是计算到纹理原本的颜色和颜色矩阵相乘得到的颜色之后，根据intensity参数得到一个原始颜色和新颜色按一定比例相加的颜色。可以看出GPUImageColorMatrixFilter这个滤镜就是使用颜色矩阵处理纹理的滤镜，而GPUImageSepiaFilter滤镜就是将颜色矩阵换成这个特效专用的矩阵。整个filter的初始化过程，就是编译shader、创建progrem、链接并使用的过程。处理图片过程将滤镜加入GPUImagePicture的targets- (void)addTarget:(id&lt;GPUImageInput&gt;)newTarget;{    NSInteger nextAvailableTextureIndex = [newTarget nextAvailableTextureIndex];    [self addTarget:newTarget atTextureLocation:nextAvailableTextureIndex];        if ([newTarget shouldIgnoreUpdatesToThisTarget])    {        _targetToIgnoreForUpdates = newTarget;    }}- (void)addTarget:(id&lt;GPUImageInput&gt;)newTarget atTextureLocation:(NSInteger)textureLocation;{    if([targets containsObject:newTarget])    {        return;    }        cachedMaximumOutputSize = CGSizeZero;    runSynchronouslyOnVideoProcessingQueue(^{        [self setInputFramebufferForTarget:newTarget atIndex:textureLocation];        [targets addObject:newTarget];        [targetTextureIndices addObject:[NSNumber numberWithInteger:textureLocation]];                allTargetsWantMonochromeData = allTargetsWantMonochromeData &amp;&amp; [newTarget wantsMonochromeInput];    });}GPUImagePicture继承自GPUImageOutput，它有一个targets的数组，里面按顺序存放实现了GPUImageInput协议的对象，形成GPUImage的响应链。当要添加一个target时，会去看这个target的纹理索引，由于GPUImageSepiaFilter是单个纹理的滤镜，所以索引是0。- (void)addTarget:(id&lt;GPUImageInput&gt;)newTarget atTextureLocation:(NSInteger)textureLocation;{    if([targets containsObject:newTarget])    {        return;    }        cachedMaximumOutputSize = CGSizeZero;    runSynchronouslyOnVideoProcessingQueue(^{        [self setInputFramebufferForTarget:newTarget atIndex:textureLocation];        [targets addObject:newTarget];        [targetTextureIndices addObject:[NSNumber numberWithInteger:textureLocation]];                allTargetsWantMonochromeData = allTargetsWantMonochromeData &amp;&amp; [newTarget wantsMonochromeInput];    });}然后将target存入targets数组，纹理索引存入targetTextureIndices，然后设置当前的outputFramebuffer为这个target的inputFramebuffer。滤镜处理- (void)useNextFrameForImageCapture;{    usingNextFrameForImageCapture = YES;    // Set the semaphore high, if it isn't already    if (dispatch_semaphore_wait(imageCaptureSemaphore, DISPATCH_TIME_NOW) != 0)    {        return;    }}首先给filter一个标记，这个值和后续一些信号量操作有关。- (void)processImage;{    [self processImageWithCompletionHandler:nil];}- (BOOL)processImageWithCompletionHandler:(void (^)(void))completion;{    hasProcessedImage = YES;        //    dispatch_semaphore_wait(imageUpdateSemaphore, DISPATCH_TIME_FOREVER);        if (dispatch_semaphore_wait(imageUpdateSemaphore, DISPATCH_TIME_NOW) != 0)    {        return NO;    }        runAsynchronouslyOnVideoProcessingQueue(^{                for (id&lt;GPUImageInput&gt; currentTarget in targets)        {            NSInteger indexOfObject = [targets indexOfObject:currentTarget];            NSInteger textureIndexOfTarget = [[targetTextureIndices objectAtIndex:indexOfObject] integerValue];                        [currentTarget setCurrentlyReceivingMonochromeInput:NO];            [currentTarget setInputSize:pixelSizeOfImage atIndex:textureIndexOfTarget];            [currentTarget setInputFramebuffer:outputFramebuffer atIndex:textureIndexOfTarget];            [currentTarget newFrameReadyAtTime:kCMTimeIndefinite atIndex:textureIndexOfTarget];        }                dispatch_semaphore_signal(imageUpdateSemaphore);                if (completion != nil) {            completion();        }    });        return YES;}首先根据信号量判断是否等待，如果等待直接返回。然后遍历所有target，将outputFramebuffer传给target。- (void)newFrameReadyAtTime:(CMTime)frameTime atIndex:(NSInteger)textureIndex;{    static const GLfloat imageVertices[] = {        -1.0f, -1.0f,        1.0f, -1.0f,        -1.0f,  1.0f,        1.0f,  1.0f,    };        [self renderToTextureWithVertices:imageVertices textureCoordinates:[[self class] textureCoordinatesForRotation:inputRotation]];    [self informTargetsAboutNewFrameAtTime:frameTime];}- (void)renderToTextureWithVertices:(const GLfloat *)vertices textureCoordinates:(const GLfloat *)textureCoordinates;{    if (self.preventRendering)    {        [firstInputFramebuffer unlock];        return;    }        [GPUImageContext setActiveShaderProgram:filterProgram];    outputFramebuffer = [[GPUImageContext sharedFramebufferCache] fetchFramebufferForSize:[self sizeOfFBO] textureOptions:self.outputTextureOptions onlyTexture:NO];    [outputFramebuffer activateFramebuffer];    if (usingNextFrameForImageCapture)    {        [outputFramebuffer lock];    }    [self setUniformsForProgramAtIndex:0];        glClearColor(backgroundColorRed, backgroundColorGreen, backgroundColorBlue, backgroundColorAlpha);    glClear(GL_COLOR_BUFFER_BIT);	glActiveTexture(GL_TEXTURE2);	glBindTexture(GL_TEXTURE_2D, [firstInputFramebuffer texture]);		glUniform1i(filterInputTextureUniform, 2);	    glVertexAttribPointer(filterPositionAttribute, 2, GL_FLOAT, 0, 0, vertices);	glVertexAttribPointer(filterTextureCoordinateAttribute, 2, GL_FLOAT, 0, 0, textureCoordinates);        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);        [firstInputFramebuffer unlock];        if (usingNextFrameForImageCapture)    {        dispatch_semaphore_signal(imageCaptureSemaphore);    }}这里filter传入顶点和纹理坐标，然后取得filter的outputFramebuffer，绑定这个buffer的帧缓冲，将从picture得到的buffer的纹理绑定和绘制，这样帧缓冲中就有了渲染后的数据。后面filter还会检查它的targets数组中是否有内容，有的话也会执行类似过程。将图片从framebuffer中取出- (CGImageRef)newCGImageFromCurrentlyProcessedOutput{    // Give it three seconds to process, then abort if they forgot to set up the image capture properly    double timeoutForImageCapture = 3.0;    dispatch_time_t convertedTimeout = dispatch_time(DISPATCH_TIME_NOW, timeoutForImageCapture * NSEC_PER_SEC);    if (dispatch_semaphore_wait(imageCaptureSemaphore, convertedTimeout) != 0)    {        return NULL;    }    GPUImageFramebuffer* framebuffer = [self framebufferForOutput];        usingNextFrameForImageCapture = NO;    dispatch_semaphore_signal(imageCaptureSemaphore);        CGImageRef image = [framebuffer newCGImageFromFramebufferContents];    return image;}这里就是使用filter的outputFramebuffer生成CGImage。- (CGImageRef)newCGImageFromFramebufferContents;{	...	__block CGImageRef cgImageFromBytes;        runSynchronouslyOnVideoProcessingQueue(^{        [GPUImageContext useImageProcessingContext];                NSUInteger totalBytesForImage = (int)_size.width * (int)_size.height * 4;        // It appears that the width of a texture must be padded out to be a multiple of 8 (32 bytes) if reading from it using a texture cache                GLubyte *rawImagePixels;                CGDataProviderRef dataProvider = NULL;           		...   		[self activateFramebuffer];       rawImagePixels = (GLubyte *)malloc(totalBytesForImage);       glReadPixels(0, 0, (int)_size.width, (int)_size.height, GL_RGBA, GL_UNSIGNED_BYTE, rawImagePixels);       dataProvider = CGDataProviderCreateWithData(NULL, rawImagePixels, totalBytesForImage, dataProviderReleaseCallback);       [self unlock]; // Don't need to keep this around anymore                    ...        cgImageFromBytes = CGImageCreate((int)_size.width, (int)_size.height, 8, 32, 4 * (int)_size.width, defaultRGBColorSpace, kCGBitmapByteOrderDefault | kCGImageAlphaLast, dataProvider, NULL, NO, kCGRenderingIntentDefault);                ...        // Capture image with current device orientation        CGDataProviderRelease(dataProvider);        CGColorSpaceRelease(defaultRGBColorSpace);            });	...	}               这个方法就是绑定这个framebuffer的帧缓冲，然后使用glReadPixels函数获取到帧缓冲中的数据，然后使用数据生成CGImage返回。至此整个滤镜处理流程完毕。总结整个处理过程中，GPUImagePicture接收图片数据并写入纹理并创建帧缓冲，GPUImageFilter负责编译、链接着色器程序，并将顶点、纹理坐标数据和图像传送到着色器，然后再有GPUImageFramebuffer将帧缓冲中的数据取出。整个源码看下来，觉得不愧是老牌图形处理库，将OpenGL封装的很好，并且设计响应链很巧妙，GPUImageFilter即是继承自GPUImageOutput，又实现了GPUImageInput协议，是的filter既可以做输入、又可以做输出。也有一些还不太确定的地方，例如filter中信号量的作用，就没太看明白，可能处理单张图片的时候还体现不出作用，后续还要继续研究。]]></content>
      <categories>
        
          <category> GPUImage </category>
        
      </categories>
      <tags>
        
          <tag> OpenGL </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
          <tag> GPUImage </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OpenGL ES iOS笔记04 使用AVSampleBufferDisplayLayer渲染视频]]></title>
      <url>/opengl/2018/04/14/OpenGLiOS04/</url>
      <content type="text"><![CDATA[上一篇笔记使用OpenGL渲染视频数据需要了解OpenGL的各种操作、函数调用，不是很符合OC习惯，如果只是简单的渲染显示功能，也可以使用苹果在iOS8中新推出的AVSampleBufferDisplayLayer类。这里还是以iOS设备摄像头采集的数据做显示。创建AVSampleBufferDisplayLayer根据名字可以知道AVSampleBufferDisplayLayer是CALayer的子类，因此可以添加到UIView的layer上，这里创建一个新的View，用于以AVSampleBufferDisplayLayer渲染视频。@interface LYRSampleBufferDisplayView ()@property(nonatomic,strong)AVSampleBufferDisplayLayer*displayLayer;@end@implementation LYRSampleBufferDisplayView-(instancetype)initWithFrame:(CGRect)frame{    if (self = [super initWithFrame:frame]) {        [self createLayer];    }    return self;}-(instancetype)initWithCoder:(NSCoder *)aDecoder{    if (self = [super initWithCoder:aDecoder]) {        [self createLayer];    }    return self;}-(void)layoutSubviews{    [super layoutSubviews];    //由于layer的frame改变时会有隐式的动画，所以需要手动禁止    [CATransaction setDisableActions:YES];    self.displayLayer.frame = self.bounds;}-(void)createLayer{    self.displayLayer = [AVSampleBufferDisplayLayer layer];    self.displayLayer.videoGravity = AVLayerVideoGravityResizeAspectFill;    [self.layer addSublayer:self.displayLayer];}@end渲染显示CMSampleBufferAVSampleBufferDisplayLayer中有方法- (void)enqueueSampleBuffer:(CMSampleBufferRef)sampleBuffer;对于从videoDataOutput中得到的数据，可以直接传入并显示。-(void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{    [self.renderView enqueueSampleBuffer:sampleBuffer];}运行程序，即可看到显示的画面。渲染普通的buffer数据当然，一般真正的应用场景中，是不会直接有CMSampleBuffer用来显示的，例如使用ffmpeg解码后的数据，一般是存放在AVFrame的结构体中，*data指针，因此，需要将buffer包装成CMSampleBuffer来给AVSampleBufferDisplayLayer显示。这里先使用RGBA的数据格式。使用CVPixelBufferPool获取CVPixelBuffer创建CMSampleBuffer，需要CVPixelBuffer，这里使用CVPixelBufferPool，这样由缓存池决定创建和销毁CVPixelBuffer，不用关心CVPixelBuffer导致的内存问题。@interface LYRSampleBufferDisplayView (){    CVPixelBufferPoolRef _pixelBufferPool;}这里创建一个方法，参数包括RGBA数据buffer指针，视频的宽高- (void)displayWithRGBBuffer:(uint8_t*)displayWithRGBBuffer linesize:(int)lineSize width:(int)width height:(int)height{    CVReturn theError;    if (_pixelBufferPool) {        CVPixelBufferPoolFlush(_pixelBufferPool, kCVPixelBufferPoolFlushExcessBuffers);        CVPixelBufferPoolRelease(_pixelBufferPool);            _pixelBufferPool = NULL;    }        if (!_pixelBufferPool){        NSMutableDictionary* attributes = [NSMutableDictionary dictionary];        //        kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange nv12        [attributes setObject:[NSNumber numberWithInt:kCVPixelFormatType_32BGRA] forKey:(NSString*)kCVPixelBufferPixelFormatTypeKey];        [attributes setObject:[NSNumber numberWithInt:width] forKey: (NSString*)kCVPixelBufferWidthKey];        [attributes setObject:[NSNumber numberWithInt:height] forKey: (NSString*)kCVPixelBufferHeightKey];        [attributes setObject:@(16) forKey:(NSString*)kCVPixelBufferBytesPerRowAlignmentKey];        [attributes setObject:[NSDictionary dictionary] forKey:(NSString*)kCVPixelBufferIOSurfacePropertiesKey];        theError = CVPixelBufferPoolCreate(kCFAllocatorDefault, NULL, (__bridge CFDictionaryRef) attributes, &amp;_pixelBufferPool);        if (theError != kCVReturnSuccess){            NSLog(@"CVPixelBufferPoolCreate Failed");            _pixelBufferPool = NULL;            return;        }    }}这里根据视频宽高和视频格式创建了一个CVPixelBufferPool。需要注意的是kCVPixelBufferBytesPerRowAlignmentKey这个参数，这个参数实际上是设置行宽（linesize/BytesPerRow）的基准而不是真正的行宽。真正的行宽会根据设置的宽高和视频格式决定，例如32BGRA的linesize一般是width*4。创建CVPixelBuffer，并将视频数据传入- (void)displayWithRGBBuffer:(uint8_t*)displayWithRGBBuffer width:(int)width height:(int)height{	...	CVPixelBufferRef pixelBuffer = nil;    theError = CVPixelBufferPoolCreatePixelBuffer(NULL, _pixelBufferPool, &amp;pixelBuffer);    if(theError != kCVReturnSuccess){        NSLog(@"CVPixelBufferPoolCreatePixelBuffer Failed");        pixelBuffer = NULL;        return;    }        CVPixelBufferLockBaseAddress(pixelBuffer, 0);        void*base = CVPixelBufferGetBaseAddress(pixelBuffer);        //将数据拷贝到base    memcpy(base, RGBBuffer, width * height *4);    if (base == NULL) {        return;    }    CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);}使用CVPixelBuffer创建CMSampleBuffer并显示数据- (void)displayPixelBuffer:(CVPixelBufferRef) pixelBuffer{    if (!pixelBuffer){        return;    }        //不设置具体时间信息    CMSampleTimingInfo timing = {kCMTimeInvalid, kCMTimeInvalid, kCMTimeInvalid};    //获取视频信息    CMVideoFormatDescriptionRef videoInfo = NULL;    OSStatus result = CMVideoFormatDescriptionCreateForImageBuffer(NULL, pixelBuffer, &amp;videoInfo);    NSParameterAssert(result == 0 &amp;&amp; videoInfo != NULL);        CMSampleBufferRef sampleBuffer = NULL;    result = CMSampleBufferCreateForImageBuffer(kCFAllocatorDefault,pixelBuffer, true, NULL, NULL, videoInfo, &amp;timing, &amp;sampleBuffer);    NSParameterAssert(result == 0 &amp;&amp; sampleBuffer != NULL);    CFRelease(pixelBuffer);    CFRelease(videoInfo);        CFArrayRef attachments = CMSampleBufferGetSampleAttachmentsArray(sampleBuffer, YES);    CFMutableDictionaryRef dict = (CFMutableDictionaryRef)CFArrayGetValueAtIndex(attachments, 0);    CFDictionarySetValue(dict, kCMSampleAttachmentKey_DisplayImmediately, kCFBooleanTrue);        //这里是处理进入后台后layer失效问题    if (self.displayLayer.status == AVQueuedSampleBufferRenderingStatusFailed) {        [self.displayLayer flush];    }        [self.displayLayer enqueueSampleBuffer:sampleBuffer];    CFRelease(sampleBuffer);}这样就完成了整个过程，接下来在采集数据的地方修改，将rgba数据传给渲染view。-(void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{                CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);        if(CVPixelBufferLockBaseAddress(imageBuffer, 0) == kCVReturnSuccess)    {        UInt8 *rgbBuffer = (UInt8 *)CVPixelBufferGetBaseAddress(imageBuffer);                size_t width = CVPixelBufferGetWidth(imageBuffer);        size_t height = CVPixelBufferGetHeight(imageBuffer);                [self.renderView displayWithRGBBuffer:rgbBuffer width:width height:height];    }    CVPixelBufferUnlockBaseAddress(imageBuffer, 0);}之后运行程序，就可以看到采集的视频。渲染NV12格式的数据这里步骤和RGBA类似，就是创建CVPixelBufferPool时格式设置为NV12，拷贝数据的时候需要两路分量都拷贝。- (void)displayWithNV12yBuffer:(uint8_t*)yBuffer uvBuffer:(uint8_t*)uvBuffer width:(int)width height:(int)height{    CVReturn theError;    if (_pixelBufferPool) {        CVPixelBufferPoolFlush(_pixelBufferPool, kCVPixelBufferPoolFlushExcessBuffers);        CVPixelBufferPoolRelease(_pixelBufferPool);        _pixelBufferPool = NULL;    }        if (!_pixelBufferPool){        NSMutableDictionary* attributes = [NSMutableDictionary dictionary];        [attributes setObject:[NSNumber numberWithInt:kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange] forKey:(NSString*)kCVPixelBufferPixelFormatTypeKey];        [attributes setObject:[NSNumber numberWithInt:width] forKey: (NSString*)kCVPixelBufferWidthKey];        [attributes setObject:[NSNumber numberWithInt:height] forKey: (NSString*)kCVPixelBufferHeightKey];        [attributes setObject:@(16) forKey:(NSString*)kCVPixelBufferBytesPerRowAlignmentKey];        [attributes setObject:[NSDictionary dictionary] forKey:(NSString*)kCVPixelBufferIOSurfacePropertiesKey];        theError = CVPixelBufferPoolCreate(kCFAllocatorDefault, NULL, (__bridge CFDictionaryRef) attributes, &amp;_pixelBufferPool);        if (theError != kCVReturnSuccess){            NSLog(@"CVPixelBufferPoolCreate Failed");            _pixelBufferPool = NULL;            return;        }    }        CVPixelBufferRef pixelBuffer = nil;    theError = CVPixelBufferPoolCreatePixelBuffer(NULL, _pixelBufferPool, &amp;pixelBuffer);    if(theError != kCVReturnSuccess){        NSLog(@"CVPixelBufferPoolCreatePixelBuffer Failed");        pixelBuffer = NULL;        return;    }        CVPixelBufferLockBaseAddress(pixelBuffer, 0);    //取得buffer中存储视频数据的指针 根据通道序号取，0是y分量，1是uv分量    void*y_base = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, 0);        //将数据拷贝到base    memcpy(y_base, yBuffer, width * height *1);//y通道的数据大小为宽乘以高    if (y_base == NULL) {        CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);        return;    }        void*uv_base = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, 1);        memcpy(uv_base, uvBuffer, width * height *0.5);//uv通道的数据大小为宽乘以高的一半        CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);        [self displayPixelBuffer:pixelBuffer];    }调用时先修改采集格式，然后依次取出两个分量buffer即可。-(void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);        if(CVPixelBufferLockBaseAddress(imageBuffer, 0) == kCVReturnSuccess)    {        //图像宽度（像素）        size_t pixelWidth = CVPixelBufferGetWidth(imageBuffer);        //图像高度（像素）        size_t pixelHeight = CVPixelBufferGetHeight(imageBuffer);        //获取CVImageBufferRef中的y数据        uint8_t *y_frame = CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 0);        //获取CMVImageBufferRef中的uv数据        uint8_t *uv_frame = CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 1);                [self.renderView displayWithNV12yBuffer:y_frame uvBuffer:uv_frame width:pixelWidth height:pixelHeight];    }    CVPixelBufferUnlockBaseAddress(imageBuffer, 0);}渲染yuv420p渲染yuv420p和上面类似，可以将CVPixelBufferPool的格式设置为kCVPixelFormatType_420YpCbCr8Planar，然后CVPixelBuffer依次拷贝进3个分量即可，这里就不放代码了。需要注意的是，直接使用yuv420p显示，实测在ios10是无法显示的，而在ios11.3以下的ios11系统，显示有问题，画面会一闪一闪的，因此在低版本适配时需要转换成NV12格式进行显示，转换方法可以使用上一篇提到的libyuv。总结虽然AVSampleBufferDisplayLayer使用起来相对简单，但实际由于每次都有memcpy的操作，所以性能会有损失，再加上直接渲染yuv的版本限制，我个人更倾向使用OpenGL渲染的方式。demo地址参考：在iOS端使用AVSampleBufferDisplayLayer进行视频渲染]]></content>
      <categories>
        
          <category> OpenGL </category>
        
      </categories>
      <tags>
        
          <tag> OpenGL </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
          <tag> AVSampleBufferDisplayLayer </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OpenGL ES iOS笔记03 渲染视频]]></title>
      <url>/opengl/2018/04/08/OpenGLiOS03/</url>
      <content type="text"><![CDATA[上一篇笔记渲染视频实际上就是不断地把视频数据放入纹理中，让OpenGL去渲染显示，本文会利用iOS设备采集RGBA、NV12数据，然后进行渲染显示。利用AVFoundation采集视频这里创建一个AVCaptureSession采集前置摄像头视频数据，格式为32BGRA，然后利用AVCaptureVideoDataOutput的代理方法获取采集到的数据。@interface ViewController ()&lt;AVCaptureVideoDataOutputSampleBufferDelegate&gt;@property(nonatomic,strong)AVCaptureSession*session;@property (weak, nonatomic) IBOutlet LYRGLView *renderView;@end- (void)viewDidLoad {    [super viewDidLoad];    // Do any additional setup after loading the view.        self.session = [[AVCaptureSession alloc]init];    self.session.sessionPreset = AVCaptureSessionPreset640x480;    NSArray *cameras = [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo];    AVCaptureDevice*frontCamera;    for (AVCaptureDevice *device in cameras){        if (device.position == AVCaptureDevicePositionFront){            frontCamera = device;        }    }    NSError *error = nil;    AVCaptureDeviceInput *videoInput = [AVCaptureDeviceInput deviceInputWithDevice:frontCamera error:&amp;error];        [self.session addInput:videoInput];        AVCaptureVideoDataOutput *avCaptureVideoDataOutput = [[AVCaptureVideoDataOutput alloc] init];    //设置采集RGBA    NSDictionary *settings = [[NSDictionary alloc] initWithObjectsAndKeys:[NSNumber numberWithUnsignedInt:kCVPixelFormatType_32BGRA], kCVPixelBufferPixelFormatTypeKey,                              nil];        avCaptureVideoDataOutput.videoSettings = settings;    avCaptureVideoDataOutput.alwaysDiscardsLateVideoFrames = YES;    dispatch_queue_t queue = dispatch_queue_create("myQueue", NULL);    [avCaptureVideoDataOutput setSampleBufferDelegate:self queue:queue];    [self.session addOutput:avCaptureVideoDataOutput];        [self.session startRunning];}AVCaptureVideoDataOutput代理方法实现-(void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);        if(CVPixelBufferLockBaseAddress(imageBuffer, 0) == kCVReturnSuccess)    {    	//获取数据地址        UInt8 *bufferPtr = (UInt8 *)CVPixelBufferGetBaseAddress(imageBuffer);        size_t width = CVPixelBufferGetWidth(imageBuffer);        size_t height = CVPixelBufferGetHeight(imageBuffer);         height:height];    }    CVPixelBufferUnlockBaseAddress(imageBuffer, 0);}bufferPtr指针即指向RGBA数据的地址。修改GLView调用OpenGL的准备工作这里将调用OpenGL的准备方法整合到一起，并在initWithFrame:和initWithCoder:中都调用它，这样在代码和storyboard中创建View就都会准备好OpenGL。@interface LYRGLView (){    GLuint _renderBuffer;    GLuint _framebuffer;        //纹理缓冲    GLuint _rgbTexture;        //着色器程序    GLuint _glprogram;    //记录renderbuffer的宽高    GLint           _backingWidth;    GLint           _backingHeight;            dispatch_queue_t _renderQueue;        //纹理参数    GLint _inputTexture;    //顶点参数    GLint _vertexPosition;    //纹理坐标参数    GLint _textureCoords;}@property(nonatomic,strong)CAEAGLLayer*eaglLayer;@property(nonatomic,strong)EAGLContext*context;@end@implementation LYRGLView#pragma mark - life cycle-(instancetype)initWithCoder:(NSCoder *)aDecoder{    if (self = [super initWithCoder:aDecoder]) {        [self commonInit];    }        return self;}-(instancetype)initWithFrame:(CGRect)frame{    if (self = [super initWithFrame:frame]) {        [self commonInit];    }    return self;}-(void)commonInit{        _renderQueue = dispatch_queue_create("renderQueue", DISPATCH_QUEUE_SERIAL);            [self prepareLayer];    dispatch_sync(_renderQueue, ^{        [self prepareContext];        [self prepareShader];        [self prepareRenderBuffer];        [self prepareFrameBuffer];            });}-(void)prepareLayer{    self.eaglLayer = (CAEAGLLayer*)self.layer;    //设置不透明，节省性能    self.eaglLayer.opaque = YES;    self.eaglLayer.drawableProperties = [NSDictionary dictionaryWithObjectsAndKeys:[NSNumber numberWithBool:NO], kEAGLDrawablePropertyRetainedBacking, kEAGLColorFormatRGBA8, kEAGLDrawablePropertyColorFormat, nil];}-(void)prepareContext{                self.context = [[EAGLContext alloc]initWithAPI:kEAGLRenderingAPIOpenGLES2];        [EAGLContext setCurrentContext:self.context];        }-(void)prepareRenderBuffer{    glGenRenderbuffers(1, &amp;_renderBuffer);    glBindRenderbuffer(GL_RENDERBUFFER, _renderBuffer);    //调用这个方法来创建一块空间用于存储缓冲数据，替代了glRenderbufferStorage    [self.context renderbufferStorage:GL_RENDERBUFFER fromDrawable:self.eaglLayer];        glGetRenderbufferParameteriv(GL_RENDERBUFFER, GL_RENDERBUFFER_WIDTH, &amp;_backingWidth);    glGetRenderbufferParameteriv(GL_RENDERBUFFER, GL_RENDERBUFFER_HEIGHT, &amp;_backingHeight);}-(void)prepareFrameBuffer{    glGenFramebuffers(1, &amp;_framebuffer);    glBindFramebuffer(GL_FRAMEBUFFER, _framebuffer);        //设置gl渲染窗口大小    glViewport(0, 0, _backingWidth, _backingHeight);    //附加之前的_renderBuffer    //GL_COLOR_ATTACHMENT0指定第一个颜色缓冲区附着点    glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0,                              GL_RENDERBUFFER, _renderBuffer);}-(void)prepareShader{    //创建顶点着色器    GLuint vertexShader = glCreateShader(GL_VERTEX_SHADER);        const GLchar* const vertexShaderSource =  (GLchar*)[vertexShaderString UTF8String];    GLint vertexShaderLength = (GLint)[vertexShaderString length];    //读取shader字符串    glShaderSource(vertexShader, 1, &amp;vertexShaderSource, &amp;vertexShaderLength);    //编译shader    glCompileShader(vertexShader);        GLint logLength;    glGetShaderiv(vertexShader, GL_INFO_LOG_LENGTH, &amp;logLength);    if (logLength &gt; 0)    {        GLchar *log = (GLchar *)malloc(logLength);        glGetShaderInfoLog(vertexShader, logLength, &amp;logLength, log);        NSLog(@"%s\n",log);        free(log);    }        //创建片元着色器    GLuint fragmentShader = glCreateShader(GL_FRAGMENT_SHADER);    const GLchar* const fragmentShaderSource = (GLchar*)[fragmentShaderString UTF8String];    GLint fragmentShaderLength = (GLint)[fragmentShaderString length];    glShaderSource(fragmentShader, 1, &amp;fragmentShaderSource, &amp;fragmentShaderLength);    glCompileShader(fragmentShader);        glGetShaderiv(fragmentShader, GL_INFO_LOG_LENGTH, &amp;logLength);    if (logLength &gt; 0)    {        GLchar *log = (GLchar *)malloc(logLength);        glGetShaderInfoLog(fragmentShader, logLength, &amp;logLength, log);        NSLog(@"%s\n",log);        free(log);    }        //创建glprogram    _glprogram = glCreateProgram();        //绑定shader    glAttachShader(_glprogram, vertexShader);    glAttachShader(_glprogram, fragmentShader);    //链接program    glLinkProgram(_glprogram);        //选择程序对象为当前使用的程序，类似setCurrentContext    glUseProgram(_glprogram);        //获取并保存参数位置    _inputTexture = glGetUniformLocation(_glprogram, "inputTexture");    _vertexPosition = glGetAttribLocation(_glprogram, "vertexPosition");    _textureCoords = glGetAttribLocation(_glprogram, "textureCoords");            //使参数可见    glEnableVertexAttribArray(_vertexPosition);    glEnableVertexAttribArray(_textureCoords);}...@end这里创建了一个串行队列，所有的OpenGL的操作都将同步在这个队列中执行。shader程序这里的shader程序和上一篇笔记中的shader基本一致，就是顶点着色器将顶点坐标和纹理坐标传给片段着色器，然后片段着色器根据纹理和纹理坐标算出每个像素的颜色。NSString *const vertexShaderString = SHADER_STRING( //attribute 关键字用来描述传入shader的变量 attribute vec4 vertexPosition; //传入的顶点坐标 attribute vec2 textureCoords;//要获取的纹理坐标 //传给片段着色器参数 varying  vec2 textureCoordsOut; void main(void) {     gl_Position = vertexPosition; // gl_Position是vertex shader的内建变量，gl_Position中的顶点值最终输出到渲染管线中     textureCoordsOut = textureCoords; } );//片段着色器NSString *const fragmentShaderString = SHADER_STRING( varying highp vec2 textureCoordsOut;  uniform sampler2D inputTexture; void main(void) {     //gl_FragColor是fragment shader的内建变量，gl_FragColor中的像素值最终输出到渲染管线中     gl_FragColor = texture2D(inputTexture, textureCoordsOut); } );使用视频数据创建纹理并渲染显示-(void)renderWithRGBData:(char*)RGBData width:(int)width height:(int)height {    dispatch_sync(_renderQueue, ^{        //检查context        if ([EAGLContext currentContext] != self.context)        {            [EAGLContext setCurrentContext:self.context];        }                GLfloat vertices[] = {            -1,-1,            1,-1,            -1,1,            1,1,                    };        GLfloat textCoord[] = {            0.0f, 1.0f,            1.0f, 1.0f,            0.0f, 0.0f,            1.0f, 0.0f,        };            glBindTexture(GL_TEXTURE_2D, _rgbTexture);        glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, GL_BGRA, GL_UNSIGNED_BYTE, RGBData);                    //设置一些边缘的处理        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);                //确定采样器对应的哪个纹理，由于只使用一个，所以这句话可以不写        glUniform1i(_inputTexture,0);        glVertexAttribPointer(_vertexPosition, 2, GL_FLOAT, GL_FALSE, 0, vertices);        glVertexAttribPointer(_textureCoords, 2, GL_FLOAT, GL_FALSE,0, textCoord);            //清屏为白色        glClearColor(1.0, 1.0, 1.0, 1.0);        glClear(GL_COLOR_BUFFER_BIT);        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);        //EACAGLContext 渲染OpenGL绘制好的图像到EACAGLLayer        [_context presentRenderbuffer:GL_RENDERBUFFER];    });}这里首先检查了EAGLContext是否是对应的context，因为context创建可能在不同的线程，导致不对应。接下来使用glTexImage2D函数将视频数据写入纹理缓冲，其中，由于采集的数据是32BGRA，所以倒数第三个参数传入GL_BGRA。之后使用glDrawArrays函数，传入GL_TRIANGLE_STRIP进行绘制，这个参数会使OpenGL逐个使用顶点并绘制三角形，根据GL_TRIANGLE_STRIP的特点，顶点坐标和纹理坐标以’Z’字形的顺序存放就可以画出矩形。GL_TRIANGLE_STRIP的绘制在AVCaptureVideoDataOutput代理中将数据传给GLView-(void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);        if(CVPixelBufferLockBaseAddress(imageBuffer, 0) == kCVReturnSuccess)    {    	//获取数据地址        UInt8 *bufferPtr = (UInt8 *)CVPixelBufferGetBaseAddress(imageBuffer);        size_t width = CVPixelBufferGetWidth(imageBuffer);        size_t height = CVPixelBufferGetHeight(imageBuffer);         height:height];         [self.renderView renderWithRGBData:bufferPtr width:width height:height];    }    CVPixelBufferUnlockBaseAddress(imageBuffer, 0);}将一个GLView添加到Controller并真机运行后，即可看到采集的画面。渲染yuv420piOS系统可采集的另一种视频格式即是NV12，也叫yuv420sp，即是y分量存在一个平面，另外两个分量交替存储在另一个分量。因为NV12相对比较少见，实际渲染时也和yuv420p有些区别，因此先实现yuv420p的渲染，而因为iOS系统不支持直接采集yuv420p，因此此处使用一个三方库，libyuv，来对NV12的数据进行转换，然后再进行渲染。修改采集格式NSDictionary *settings = [[NSDictionary alloc] initWithObjectsAndKeys:[NSNumber numberWithUnsignedInt:kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange], kCVPixelBufferPixelFormatTypeKey,nil];这里可选kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange和kCVPixelFormatType_420YpCbCr8BiPlanarFullRange，都是采集NV12，区别是uv分量的取值范围不同，这里选择第一种。使用libyuv转换为yuv420p格式数据libyuv在videoDataOutput的代理方法中，拿到sampleBuffer，取出两个分量，然后使用libyuv的函数NV12ToI420。-(void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);        if(CVPixelBufferLockBaseAddress(imageBuffer, 0) == kCVReturnSuccess)    {        //图像宽度（像素）        size_t pixelWidth = CVPixelBufferGetWidth(imageBuffer);        //图像高度（像素）        size_t pixelHeight = CVPixelBufferGetHeight(imageBuffer);        //获取CVImageBufferRef中的y数据        uint8_t *y_frame = CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 0);        //获取CMVImageBufferRef中的uv数据        uint8_t *uv_frame = CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 1);        //y stride        size_t plane1_stride = CVPixelBufferGetBytesPerRowOfPlane (imageBuffer, 0);        //uv stride        size_t plane2_stride = CVPixelBufferGetBytesPerRowOfPlane (imageBuffer, 1);        //y_size        size_t plane1_size = plane1_stride * CVPixelBufferGetHeightOfPlane(imageBuffer, 0);        //uv_size        size_t plane2_size = CVPixelBufferGetBytesPerRowOfPlane (imageBuffer, 1) * CVPixelBufferGetHeightOfPlane(imageBuffer, 1);        //yuv_size        size_t frame_size = plane1_size + plane2_size;                //这些几个指针就是转换后的yuv分量的指针        uint8* dst_y = malloc(frame_size);        uint8* dst_u = dst_y + plane1_size;        uint8* dst_v = dst_u + plane1_size/4;                if (dst_y == NULL || dst_u == NULL || dst_v == NULL) {            CVPixelBufferUnlockBaseAddress(imageBuffer, 0);            return;        }        // Let libyuv convert        int ret = NV12ToI420(y_frame, (int)plane1_stride,                             uv_frame, (int)plane2_stride,                             dst_y, (int)plane1_stride,                             dst_u, (int)plane2_stride/2,                             dst_v, (int)plane2_stride/2,                             (int)pixelWidth, (int)pixelHeight);        if (ret &lt; 0) {            free(dst_y);            CVPixelBufferUnlockBaseAddress(imageBuffer, 0);            return;        }                		//使用完之后需要手动释放内存        free(dst_y);    }    CVPixelBufferUnlockBaseAddress(imageBuffer, 0);}修改shader因为有yuv三个分量，因此就有三个纹理，片段着色器就有三个sampler2D，顶点着色器不做改动。NSString *const fragmentShaderString = SHADER_STRING( varying highp vec2 textureCoordsOut;  uniform sampler2D y_texture; uniform sampler2D u_texture; uniform sampler2D v_texture;  void main(void) {          highp float y = texture2D(y_texture, textureCoordsOut).r;     highp float u = texture2D(u_texture, textureCoordsOut).r - 0.5 ;     highp float v = texture2D(v_texture, textureCoordsOut).r -0.5;     //yuv转换rgb公式     highp float r = y +             1.402 * v;     highp float g = y - 0.344 * u - 0.714 * v;     highp float b = y + 1.772 * u;          gl_FragColor = vec4(r,g,b,1.0);      } );修改纹理@interface LYRYUVView (){    ...    //纹理缓冲    GLuint _yTexture;    GLuint _uTexture;    GLuint _vTexture;        ...        //纹理参数    GLint _y_texture;    GLint _u_texture;    GLint _v_texture;    }...-(void)prepareShader{    ...    //选择程序对象为当前使用的程序，类似setCurrentContext    glUseProgram(_glprogram);        //获取并保存参数位置    _y_texture = glGetUniformLocation(_glprogram, "y_texture");    _u_texture = glGetUniformLocation(_glprogram, "u_texture");    _v_texture = glGetUniformLocation(_glprogram, "v_texture");    //分配缓冲    glGenTextures(1, &amp;_yTexture);    glGenTextures(1, &amp;_uTexture);    glGenTextures(1, &amp;_vTexture);        _vertexPosition = glGetAttribLocation(_glprogram, "vertexPosition");   ...    }修改渲染方法，将yuv纹理传入纹理缓冲-(void)renderWithYData:(char*)YData UData:(char*)UData VData:(char*)VData width:(int)width height:(int)height{    dispatch_sync(_renderQueue, ^{        //检查context        if ([EAGLContext currentContext] != self.context)        {            [EAGLContext setCurrentContext:self.context];        }                GLfloat vertices[] = {            -1,-1,            1,-1,            -1,1,            1,1,                    };        GLfloat textCoord[] = {            0.0f, 1.0f,            1.0f, 1.0f,            0.0f, 0.0f,            1.0f, 0.0f,        };                glActiveTexture(GL_TEXTURE0);        glBindTexture(GL_TEXTURE_2D, _yTexture);        glUniform1i(_y_texture,0);        glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE, width, height, 0, GL_LUMINANCE, GL_UNSIGNED_BYTE, YData);        //设置一些边缘的处理,每个纹理都需要设置        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);                        glActiveTexture(GL_TEXTURE0 + 1);        glBindTexture(GL_TEXTURE_2D, _uTexture);        glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE, width/2, height/2, 0, GL_LUMINANCE, GL_UNSIGNED_BYTE, UData);        glUniform1i(_u_texture,1);        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);                        glActiveTexture(GL_TEXTURE0 + 2);        glBindTexture(GL_TEXTURE_2D, _vTexture);        glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE, width/2, height/2, 0, GL_LUMINANCE, GL_UNSIGNED_BYTE, VData);        glUniform1i(_v_texture,2);                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);                        glVertexAttribPointer(_vertexPosition, 2, GL_FLOAT, GL_FALSE, 0, vertices);        glVertexAttribPointer(_textureCoords, 2, GL_FLOAT, GL_FALSE,0, textCoord);                //清屏为白色        glClearColor(1.0, 1.0, 1.0, 1.0);        glClear(GL_COLOR_BUFFER_BIT);                glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);        //EACAGLContext 渲染OpenGL绘制好的图像到EACAGLLayer        [_context presentRenderbuffer:GL_RENDERBUFFER];    });}之后调用方法，运行后即可看到采集的视频。-(void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{   ...        // Let libyuv convert        int ret = NV12ToI420(y_frame, (int)plane1_stride,                             uv_frame, (int)plane2_stride,                             dst_y, (int)plane1_stride,                             dst_u, (int)plane2_stride/2,                             dst_v, (int)plane2_stride/2,                             (int)pixelWidth, (int)pixelHeight);        if (ret &lt; 0) {            free(dst_y);            CVPixelBufferUnlockBaseAddress(imageBuffer, 0);            return;        }                        [self.renderView renderWithYData:dst_y UData:dst_u VData:dst_v width:pixelWidth height:pixelHeight];        //使用完之后需要手动释放内存        free(dst_y);    ...}渲染NV12修改shader这里和yuv420p的区别就是传入的纹理参数是两个，第二个是uv分量，u分量从r中取，v分量从a中取。在网上一些例子中，v分量是从g中获取的，这个应该是移动端和PC端存储数据位置的不同。NSString *const fragmentShaderString = SHADER_STRING( varying highp vec2 textureCoordsOut;  uniform sampler2D y_texture; uniform sampler2D uv_texture;  void main(void) {          highp float y = texture2D(y_texture, textureCoordsOut).r;     highp float u = texture2D(uv_texture, textureCoordsOut).r - 0.5 ;     highp float v = texture2D(uv_texture, textureCoordsOut).a -0.5;     highp float r = y +             1.402 * v;     highp float g = y - 0.344 * u - 0.714 * v;     highp float b = y + 1.772 * u;     gl_FragColor = vec4(r,g,b,1.0); } );修改纹理@interface LYRNV12View (){	...    //纹理缓冲    GLuint _yTexture;    GLuint _uvTexture;        ...        //纹理参数    GLint _y_texture;    GLint _uv_texture;    ...}-(void)prepareShader{    ...        //选择程序对象为当前使用的程序，类似setCurrentContext    glUseProgram(_glprogram);        //获取并保存参数位置    _y_texture = glGetUniformLocation(_glprogram, "y_texture");    _uv_texture = glGetUniformLocation(_glprogram, "uv_texture");        glGenTextures(1, &amp;_yTexture);    glGenTextures(1, &amp;_uvTexture);        _vertexPosition = glGetAttribLocation(_glprogram, "vertexPosition");   ...         }修改渲染方法-(void)renderWithYData:(char*)YData UVData:(char*)UVData width:(int)width height:(int)height{    dispatch_sync(_renderQueue, ^{        //检查context        if ([EAGLContext currentContext] != self.context)        {            [EAGLContext setCurrentContext:self.context];        }                GLfloat vertices[] = {            -1,1,            1,1,            -1,-1,            1,-1,                    };        GLfloat textCoord[] = {            0,1,            1,1,            0,0,            1,0,        };        glActiveTexture(GL_TEXTURE0);        glBindTexture(GL_TEXTURE_2D, _yTexture);                glUniform1i(_y_texture,0);        glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE, width, height, 0, GL_LUMINANCE, GL_UNSIGNED_BYTE, YData);        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);                        glActiveTexture(GL_TEXTURE0 + 1);        glBindTexture(GL_TEXTURE_2D, _uvTexture);        //uv分量的参数变为GL_LUMINANCE_ALPHA        glTexImage2D(GL_TEXTURE_2D, 0, GL_LUMINANCE_ALPHA, width/2, height/2, 0, GL_LUMINANCE_ALPHA, GL_UNSIGNED_BYTE, UVData);        glUniform1i(_uv_texture,1);                        //设置一些边缘的处理        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);                                        glVertexAttribPointer(_vertexPosition, 2, GL_FLOAT, GL_FALSE, 0, vertices);        glVertexAttribPointer(_textureCoords, 2, GL_FLOAT, GL_FALSE,0, textCoord);                //清屏为白色        glClearColor(1.0, 1.0, 1.0, 1.0);        glClear(GL_COLOR_BUFFER_BIT);                glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);        //EACAGLContext 渲染OpenGL绘制好的图像到EACAGLLayer        [_context presentRenderbuffer:GL_RENDERBUFFER];    });}这里需要注意，uv分量的参数变为GL_LUMINANCE_ALPHA，这个参数表示按照亮度和alpha值存储纹理单元，而GL_LUMINANCE表示按照亮度值存储纹理单元，alpha值固定为1。在修改shader时也提到，v分量是从a中获取的，也就是说如果使用GL_LUMINANCE，则v分量的值会丢失。之后调用方法，运行后即可看到采集的视频。-(void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection{	UInt8 *yBuffer = (UInt8 *)CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 0);	UInt8 *uvBuffer = (UInt8 *)CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 1);	size_t width = CVPixelBufferGetWidth(imageBuffer);	size_t height = CVPixelBufferGetHeight(imageBuffer);	self.renderView renderWithYData:yBuffer UVData:uvBuffer width:width height:height];	}总结视频渲染是OpenGL很重要的一个功能，在学习研究的时候也发现不少坑，主要是一些细节的参数的不同，以及移动端和PC端的差异导致的，其中看kxmovie和GPUImage的源码还是收益不少的。demo地址参考：kxmovieGPUImageopengl渲染nv12视频]]></content>
      <categories>
        
          <category> OpenGL </category>
        
      </categories>
      <tags>
        
          <tag> OpenGL </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OpenGL ES iOS笔记02 使用纹理]]></title>
      <url>/opengl/2018/03/25/OpenGLiOS02/</url>
      <content type="text"><![CDATA[上一篇笔记纹理是OpenGL很重要的一部分，处理视频图像，基本就是在使用纹理做各种处理，本文会实现将一张图片加载到纹理中，再由OpenGL渲染到屏幕上。加载与创建纹理一般的图片格式都是压缩的，使用UIImage加载后，需要转换成位图（bitmap）,一般使用RGBA格式，本次使用的图片带有alpha通道，如果使用不带alpha通道的图片，转换位图的代码也需要相应作改动。-(void)render {	 //加载图片并转换为rgba，存放到imageData中    NSString*imagePath = [[NSBundle mainBundle]pathForResource:@"container" ofType:@"png"];    UIImage*image = [UIImage imageWithContentsOfFile:imagePath];    CGImageRef cgImageRef = [image CGImage];    GLuint width = (GLuint)CGImageGetWidth(cgImageRef);    GLuint height = (GLuint)CGImageGetHeight(cgImageRef);    CGRect rect = CGRectMake(0, 0, width, height);    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();    void *imageData = malloc(width * height * 4);    CGContextRef context = CGBitmapContextCreate(imageData, width, height, 8, width * 4, colorSpace, kCGImageAlphaPremultipliedLast | kCGBitmapByteOrder32Big);	CGContextTranslateCTM(context, 0, height);    CGContextScaleCTM(context, 1.0, -1.0);    CGColorSpaceRelease(colorSpace);    CGContextDrawImage(context, rect, cgImageRef);        //创建纹理    unsigned int texture;    glGenTextures(1, &amp;texture);    glBindTexture(GL_TEXTURE_2D, texture);        //设置一些边缘的处理    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);        //将图片数据加载到纹理中    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, image.size.width, image.size.height, 0, GL_RGBA, GL_UNSIGNED_BYTE, imageData);        //释放图片数据    CGContextRelease(context);    free(imageData);        ...}修改shader程序这里给顶点着色器添加一个参数输入纹理坐标，然后增加一个传给片段着色器的纹理坐标参数，然后片段着色器将顶点着色器传来的坐标用于获取纹理上的颜色。//顶点着色器NSString *const vertexShaderString = SHADER_STRING( //attribute 关键字用来描述传入shader的变量 attribute vec4 vertexPosition; //传入的顶点坐标 attribute vec2 textureCoords;//要获取的纹理坐标 //传给片段着色器参数 varying  vec2 textureCoordsOut; void main(void) {     gl_Position = vertexPosition; // gl_Position是vertex shader的内建变量，gl_Position中的顶点值最终输出到渲染管线中     textureCoordsOut = textureCoords; } );//片段着色器NSString *const fragmentShaderString = SHADER_STRING( varying highp vec2 textureCoordsOut;  uniform sampler2D Texture; void main(void) {     //gl_FragColor是fragment shader的内建变量，gl_FragColor中的像素值最终输出到渲染管线中     gl_FragColor = texture2D(Texture, textureCoordsOut); } );设置纹理坐标因为使用GL_TEXTURE_2D，所以纹理坐标是二维向量，在OpenGL中纹理坐标是以左下角为原点的。这里将之前的顶点坐标数组扩充，每个顶点后面跟着一个纹理坐标点。-(void)render {	... 	float vertices[] = {        // positions           // texture coords        0.5f,  0.5f, 0.0f,    1.0f, 1.0f, // top right        0.5f, -0.5f, 0.0f,    1.0f, 0.0f, // bottom right        -0.5f, -0.5f, 0.0f,   0.0f, 0.0f, // bottom left        -0.5f,  0.5f, 0.0f,   0.0f, 1.0f  // top left    };        const GLint Indices[] = {        0, 1, 3,        1, 2, 3    };    ...}将纹理坐标作为数据传给shader-(void)render {	...	//顶点坐标对象    GLuint vertexBuffer;    glGenBuffers(1, &amp;vertexBuffer);    glBindBuffer(GL_ARRAY_BUFFER, vertexBuffer);    //将顶点坐标写入顶点VBO    glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);            //索引    GLuint indexBuffer;    glGenBuffers(1, &amp;indexBuffer);    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, indexBuffer);    //将顶点索引数据写入索引缓冲对象    glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(Indices), Indices, GL_STATIC_DRAW);            GLuint vertexPosition = glGetAttribLocation(_glprogram, "vertexPosition");    glVertexAttribPointer(vertexPosition, 3, GL_FLOAT, GL_FALSE, 5 * sizeof(float), (void*)0);    glEnableVertexAttribArray(vertexPosition);            GLuint textureCoords = glGetAttribLocation(_glprogram, "textureCoords");    //vertices数组中，每五个元素取后两个作为纹理坐标    glVertexAttribPointer(textureCoords, 2, GL_FLOAT, GL_FALSE, 5 * sizeof(float), (void*)(3 * sizeof(float)));    glEnableVertexAttribArray(textureCoords);    ...}渲染显示之后再调用glDrawElements函数渲染即可显示。&lt;img src=”https://i.loli.net/2019/06/15/5d047c7d7651684651.png” width = “36%” height = “” div align= center/&gt;demo地址参考：纹理IOS 中openGL使用教程3（openGL ES 入门篇 | 纹理贴图（texture）使用）]]></content>
      <categories>
        
          <category> OpenGL </category>
        
      </categories>
      <tags>
        
          <tag> OpenGL </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[OpenGL ES iOS笔记01 使用OpenGL绘制三角形]]></title>
      <url>/opengl/2018/03/14/OpenGLiOS01/</url>
      <content type="text"><![CDATA[因为工作需要，需要了解下OpenGL的基本使用和原理，以及ios下的滤镜框架GPUImage的实现原理，因此在此记录一下。OpenGL渲染简单流程  在OpenGL中，任何事物都在3D空间中，而屏幕和窗口却是2D像素数组，这导致OpenGL的大部分工作都是关于把3D坐标转变为适应你屏幕的2D像素。为了描述3D空间中的点，就需要一个三维坐标，一个三维坐标的数组叫做顶点数据（Vertex Data），这些顶点数据描述了需要绘制的图形的点，可以指定这些点渲染成点、线或者三角形。中间经过图元装配阶段、几何着色器处理阶段、光栅化阶段，然后进入片段着色器，来计算一个像素的最终颜色，之后还会经过Alpha测试和混合(Blending)阶段，然后得出最终的像素颜色。  图元装配(Primitive Assembly)阶段将顶点着色器输出的所有顶点作为输入（如果是GL_POINTS，那么就是一个顶点），并所有的点装配成指定图元的形状.  图元装配阶段的输出会传递给几何着色器(Geometry Shader)。几何着色器把图元形式的一系列顶点的集合作为输入，它可以通过产生新顶点构造出新的（或是其它的）图元来生成其他形状。  几何着色器的输出会被传入光栅化阶段(Rasterization Stage)，这里它会把图元映射为最终屏幕上相应的像素，生成供片段着色器(Fragment Shader)使用的片段(Fragment)。在片段着色器运行之前会执行裁切(Clipping)。裁切会丢弃超出你的视图以外的所有像素，用来提升执行效率。大多数情况下，只需要关系顶点着色器和片段着色器的处理即可。在现代OpenGL中，必须定义至少一个顶点着色器和一个片段着色器（因为GPU中没有默认的顶点/片段着色器）。在iOS环境下使用OpenGL流程创建自定义view并修改layer在iOS中一般使用CAEAGLLayer来实现OpenGL的各种功能，一般使用自定义的UIView，并改变他的layer的类型。CAEAGLLayer默认是透明的，官方建议设为不透明。@interface LYRGLView ()@property(nonatomic,strong)CAEAGLLayer*eaglLayer;@end@implementation LYRGLView+(Class)layerClass{    return [CAEAGLLayer class];}-(void)prepareLayer{    self.eaglLayer = (CAEAGLLayer*)self.layer;    //设置不透明，节省性能    self.eaglLayer.opaque = YES;}@end创建Context上下文@property(nonatomic,strong)EAGLContext*context;...-(void)prepareContext{    self.context = [[EAGLContext alloc]initWithAPI:kEAGLRenderingAPIOpenGLES2];    [EAGLContext setCurrentContext:self.context];}创建render buffer（渲染缓存）render buffer用来存储即将绘制到屏幕上的图像数据，理解为帧缓冲的一个附件，用来真正存储图像的数据。@interface LYRGLView (){    GLuint _renderBuffer;}....-(void)prepareRenderBuffer{    glGenRenderbuffers(1, &amp;_renderBuffer);    glBindRenderbuffer(GL_RENDERBUFFER, _renderBuffer);    //调用这个方法来创建一块空间用于存储缓冲数据，替代了glRenderbufferStorage    [self.context renderbufferStorage:GL_RENDERBUFFER fromDrawable:self.eaglLayer];}创建frame buffer（帧缓冲）帧缓冲理解为多种缓冲的结合。-(void)prepareFrameBuffer{    GLuint framebuffer;    glGenFramebuffers(1, &amp;framebuffer);    glBindFramebuffer(GL_FRAMEBUFFER, framebuffer);    //附加之前的_renderBuffer    //GL_COLOR_ATTACHMENT0指定第一个颜色缓冲区附着点    glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0,                              GL_RENDERBUFFER, _renderBuffer);}创建并编译shadershader使用着色器语言GLSL(OpenGL Shading Language)编写，主要需要编写顶点着色器和片段着色器的实现，顶点着色器将计算好的顶点传入片段着色器，然后片段着色器计算像素最后的颜色输出。GLSL语言基础这里定义两个shader，其中顶点shader直接将顶点坐标传给片段shader，片段shader将像素颜色固定为绿色。shader的具体代码使用NSString字符串常量保存。//方便定义shader字符串的宏#define STRINGIZE(x) #x#define STRINGIZE2(x) STRINGIZE(x)#define SHADER_STRING(text) @ STRINGIZE2(text)//顶点着色器NSString *const vertexShaderString = SHADER_STRING( //attribute 关键字用来描述传入shader的变量 attribute vec4 vertexPosition; //传入的顶点坐标 void main(void) {     gl_Position = vertexPosition; // gl_Position是vertex shader的内建变量，gl_Position中的顶点值最终输出到渲染管线中 });//片段着色器NSString *const fragmentShaderString = SHADER_STRING( void main(void) { 	//设置为绿色    gl_FragColor = vec4(0, 1, 0, 1); // gl_FragColor是fragment shader的内建变量，gl_FragColor中的像素值最终输出到渲染管线中 });接下来需要使用这些字符串常量创建并编译-(void)prepareShader{    //创建顶点着色器    GLuint vertexShader = glCreateShader(GL_VERTEX_SHADER);    const GLchar* const vertexShaderSource =  (GLchar*)[vertexShaderString UTF8String];    GLint vertexShaderLength = (GLint)[vertexShaderString length];    //读取shader字符串    glShaderSource(vertexShader, 1, &amp;vertexShaderSource, &amp;vertexShaderLength);    //编译shader    glCompileShader(vertexShader);    //创建片元着色器    GLuint fragmentShader = glCreateShader(GL_FRAGMENT_SHADER);    const GLchar* const fragmentShaderSource = (GLchar*)[fragmentShaderString UTF8String];    GLint fragmentShaderLength = (GLint)[fragmentShaderString length];    glShaderSource(fragmentShader, 1, &amp;fragmentShaderSource, &amp;fragmentShaderLength);    glCompileShader(fragmentShader);    }创建着色器程序并链接shader着色器程序对象是多个着色器合并之后并最终链接完成的版本。需要将刚才创建的shader链接至着色器程序。@interface LYRGLView (){    GLuint _renderBuffer;    //着色器程序    GLuint _glprogram;}...-(void)prepareShader{...//创建glprogram    _glprogram = glCreateProgram();        //绑定shader    glAttachShader(_glprogram, vertexShader);    glAttachShader(_glprogram, fragmentShader);    //链接program    glLinkProgram(_glprogram);        //选择程序对象为当前使用的程序，类似setCurrentContext    glUseProgram(_glprogram);传入顶点数据，完成三角形绘制-(void)render {	//shader中vertexPosition参数的索引，因为是只有一个参数，所以是0，也可以使用glGetAttribLocation函数，传入_glprogrem和参数名称字符串查找	int vertexPositionIndex = 0;	//启用attribute变量，使其对GPU可见，默认为关闭   	glEnableVertexAttribArray(vertexPositionIndex);   	   	//绘制三角形需要三个坐标，由于是屏幕，所以z的值都为0。OpenGL的坐标系是以中心为原点的，所以（1，1）在右上角   	const float vertices[] = {        1.0f, 1.0f, 0.0f,   // 右上角        1.0f, -1.0f, 0.0f,  // 右下角        -1.0f, -1.0f, 0.0f, // 左下角    };        //顶点坐标对象    GLuint vertexBuffer;    glGenBuffers(1, &amp;vertexBuffer);    glBindBuffer(GL_ARRAY_BUFFER, vertexBuffer);    //将顶点坐标写入顶点VBO    glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);    //告诉OpenGL该如何解析顶点数据    //每个顶点属性从一个VBO管理的内存中获得它的数据，而具体是从哪个VBO（程序中可以有多个VBO）获取则是通过在调用glVetexAttribPointer时绑定到GL_ARRAY_BUFFER的VBO决定的。由于在调用glVetexAttribPointer之前绑定的是先前定义的VBO对象，顶点属性0现在会链接到它的顶点数据。    glVertexAttribPointer(vertexPositionIndex, 3, GL_FLOAT, GL_FALSE, sizeof(float)*3, (void*)0);        //清屏为白色    glClearColor(1.0, 1.0, 1.0, 1.0);    glClear(GL_COLOR_BUFFER_BIT);    //设置gl渲染窗口大小    glViewport(0, 0, self.frame.size.width, self.frame.size.height);    //绘制三个顶点的三角形	glDrawArrays(GL_TRIANGLES, 0, 3);        //EACAGLContext 渲染OpenGL绘制好的图像到EACAGLLayer    [_context presentRenderbuffer:GL_RENDERBUFFER];}最后创建一个自定义view，并在init方法中调用上述方法，即可渲染出三角形。&lt;img src=”https://i.loli.net/2019/06/15/5d047c69a2de390910.png” alt=”OpenGL01三角.png” title=”OpenGL01三角.png” width=36% /&gt;使用索引缓冲对象画一个矩形画一个占据屏幕的矩形，需要两个三角形，也就是6个顶点，可以将之前的顶点坐标数据修改,再将绘制是的顶点数量由3修改为6即可。float vertices[] = {    // 第一个三角形    1.0f, 1.0f, 0.0f,   // 右上角    1.0f, -1.0f, 0.0f,  // 右下角    -1.0f, 1.0f, 0.0f,  // 左上角    // 第二个三角形    1.0f, -1.0f, 0.0f,  // 右下角    -1.0f, -1.0f, 0.0f, // 左下角    -1.0f, 1.0f, 0.0f   // 左上角};...glDrawArrays(GL_TRIANGLES, 0, 6);&lt;img src=”https://i.loli.net/2019/06/15/5d047c686d22211739.png” alt=”OpenGL01矩形.png” title=”OpenGL01矩形.png” width= 36%/&gt;这样就会发现，6个顶点中有两个顶点是重复的，如果绘制成千上万个图形，带来的内存开支是非常多的，这时候就可以使用索引缓冲对象，顶点数组存放不重复的顶点，然后将顶点的索引以绘制的形状来存储，可以避免上面的问题。使用索引绘制-(void)render {    ...    float vertices[] = {        1.0f, 1.0f, 0.0f,   // 右上角        1.0f, -1.0f, 0.0f,  // 右下角        -1.0f, 1.0f, 0.0f,  // 左上角        -1.0f, -1.0f, 0.0f, // 左下角    };        const GLint Indices[] = {        0, 1, 2,        2, 3, 1    };        //顶点坐标对象    GLuint vertexBuffer;    glGenBuffers(1, &amp;vertexBuffer);    glBindBuffer(GL_ARRAY_BUFFER, vertexBuffer);    //将顶点坐标写入顶点VBO    glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);            //索引    GLuint indexBuffer;    glGenBuffers(1, &amp;indexBuffer);    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, indexBuffer);    //将顶点索引数据写入索引缓冲对象    glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(Indices), Indices, GL_STATIC_DRAW);        ...    	//    glDrawArrays(GL_TRIANGLES, 0, 6);    glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0);    }运行之后，也可以显示出绿色的矩形。demo地址参考：你好，三角形GLSL语言基础OpenGL ES Programming Guide]]></content>
      <categories>
        
          <category> OpenGL </category>
        
      </categories>
      <tags>
        
          <tag> OpenGL </tag>
        
          <tag> 音视频 </tag>
        
          <tag> 笔记 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
